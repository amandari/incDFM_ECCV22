{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as TF\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from torchvision import datasets, models, transforms\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import tforms\n",
    "import utils\n",
    "sys.path.append('../src/novelty_dfm_CL/')\n",
    "\n",
    "from dset8_specific_scripts.eightdset_wrappers.aircraft import MY_AIRCRAFT\n",
    "from dset8_specific_scripts.eightdset_wrappers.birds import MY_BIRDS\n",
    "from dset8_specific_scripts.eightdset_wrappers.cars import MY_CARS\n",
    "from dset8_specific_scripts.eightdset_wrappers.voc import MY_VOC\n",
    "from dset8_specific_scripts.eightdset_wrappers.char import MY_CHAR\n",
    "from dset8_specific_scripts.eightdset_wrappers.flowers import MY_FLOWERS\n",
    "from dset8_specific_scripts.eightdset_wrappers.scenes import MY_SCENES\n",
    "from dset8_specific_scripts.eightdset_wrappers.svhn import MY_SVHN\n",
    "\n",
    "    \n",
    "class eightdset_normalize():\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "\n",
    "\n",
    "class eightdset_train():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.RandomHorizontalFlip(), TF.ToTensor(), \n",
    "                            #   inaturalist_normalize(),\n",
    "                              ])\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "    \n",
    "transform = eightdset_train()\n",
    "\n",
    "transform_svhn = tforms.svhn_train()    \n",
    "    \n",
    "data_dir = '../data/8dset/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shuffle_loader=True\n",
    "batchsize=100\n",
    "\n",
    "trainset_task1 = MY_FLOWERS(img_path=data_dir+'/flowers',\n",
    "                                    txt_path=(data_dir+'/flowers/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='val')\n",
    "train_loader_task1 = torch.utils.data.DataLoader(trainset_task1, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "trainset_task2 = MY_SCENES(img_path=data_dir+'/scenes',\n",
    "                                    txt_path=(data_dir+'/scenes/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task2 = torch.utils.data.DataLoader(trainset_task2, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "trainset_task3 = MY_BIRDS(img_path=data_dir+'/birds',\n",
    "                                    txt_path=(data_dir+'/birds/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task3 = torch.utils.data.DataLoader(trainset_task3, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "\n",
    "# ======================\n",
    "trainset_task4 = MY_CARS(img_path=data_dir+'/cars',\n",
    "                                    txt_path=(data_dir+'/cars/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task4 = torch.utils.data.DataLoader(trainset_task4, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "trainset_task5 = MY_AIRCRAFT(img_path=data_dir+'/aircraft',\n",
    "                                    txt_path=(data_dir+'/aircraft/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task5 = torch.utils.data.DataLoader(trainset_task5, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "trainset_task6 = MY_VOC(img_path=data_dir+'/voc',\n",
    "                                    txt_path=(data_dir+'/voc/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task6 = torch.utils.data.DataLoader(trainset_task6, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "# ======================\n",
    "trainset_task7 = MY_CHAR(img_path=data_dir+'/chars',\n",
    "                                    txt_path=(data_dir+'/chars/labels/train.txt'),\n",
    "                                    data_transforms=transform,\n",
    "                                    dataset='train')\n",
    "train_loader_task7 = torch.utils.data.DataLoader(trainset_task7, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "\n",
    "# ======================\n",
    "trainset_task8 = MY_SVHN(img_path=data_dir+'/svhn',\n",
    "                                    data_transforms=transform_svhn,\n",
    "                                    split='train')\n",
    "train_loader_task8 = torch.utils.data.DataLoader(trainset_task8, batch_size=batchsize,\n",
    "                                            shuffle=shuffle_loader, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "datasets = [trainset_task1, \n",
    "            trainset_task2, \n",
    "            trainset_task3, \n",
    "            trainset_task4,\n",
    "            trainset_task5,\n",
    "            trainset_task6,\n",
    "            trainset_task7,\n",
    "            trainset_task8,]\n",
    "\n",
    "loaders = [train_loader_task1, \n",
    "            train_loader_task2, \n",
    "            train_loader_task3, \n",
    "            train_loader_task4,\n",
    "            train_loader_task5,\n",
    "            train_loader_task6,\n",
    "            train_loader_task7,\n",
    "            train_loader_task8,]\n",
    "\n",
    "\n",
    "num_classes = [102,67,200,196,70,10,62,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def eightdset_Experiments_w_holdout_w_validation_trainset(dataroot, outdir, experiment_name, dset_name='8dset',\\\n",
    "    holdout_percent=0.2, validation_percent=0.1):\n",
    "    \"\"\" \n",
    "    Only do holdout for Train\n",
    "    Args:\n",
    "        holdout_percent (float): percent of train data to leave out for later\n",
    "        max_holdout (float): maximum holdout_percent allowed. Usually not changed \n",
    "        root (string): Root directory of the dataset where images and paths file are stored\n",
    "        outdir (string): Out directory to store experiment task files (txt sequences of objects)\n",
    "        train (bool, optional): partition set. If train=True then it is train set. Else, test. \n",
    "        scenario (string, optional): What tye of CL learning regime. 'nc' stands for class incremental,\\\n",
    "             where each task contains disjoint class subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    list_tasks = ['flowers', 'scenes', 'birds', 'cars', 'aircraft', 'voc', 'chars', 'svhn']\n",
    "    individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "    \n",
    "    tasklists_train = ['%s/%s/labels/train.txt'%(dataroot,d) for d in list_tasks[:-1]]+['train']\n",
    "\n",
    "    dsets = []\n",
    "    for i, dname in enumerate(list_tasks):\n",
    "        if dname =='svhn':\n",
    "            dataset = MY_SVHN(img_path='/lab/arios/ProjIntel/incDFM/data/svhn/',\n",
    "                                    split='train')\n",
    "        else:\n",
    "            dataset = individual_wrappers[dname](img_path='%s/%s'%(dataroot, dname),\n",
    "                                            txt_path=tasklists_train[i],\n",
    "                                            dataset='train')\n",
    "        dsets.append(dataset)\n",
    "        \n",
    "                \n",
    "    # --- Set up Task sequences 8dset\n",
    "    tasks_list_train=[]\n",
    "    tasks_list_val=[]\n",
    "    tasks_list_holdout=[]\n",
    "    for t, task in enumerate(list_tasks):\n",
    "        \n",
    "        labels = np.array(dsets[t].img_label)\n",
    "        print('Dset %s - Num data %d Num labels %d'%(task, labels.shape[0], np.unique(labels).shape[0]))\n",
    "        num_samples_task = labels.shape[0]\n",
    "        indices_task = np.arange(num_samples_task)\n",
    "\n",
    "        # for each label subset\n",
    "        print('num_samples_task', num_samples_task)\n",
    "        \n",
    "        inds_shuf = np.random.permutation(indices_task)\n",
    "        \n",
    "        # divide the train/val/holdout split\n",
    "        split_val = int(np.floor(validation_percent*num_samples_task))\n",
    "        tasks_list_val.append(inds_shuf[:split_val])\n",
    "        inds_train = inds_shuf[split_val:]\n",
    "\n",
    "\n",
    "        split_holdout = int(np.floor(holdout_percent*num_samples_task))\n",
    "        tasks_list_train.append(inds_train[split_holdout:])\n",
    "        tasks_list_holdout.append(inds_train[:split_holdout])\n",
    "\n",
    "        print(tasks_list_train[-1].shape, tasks_list_val[-1].shape, tasks_list_holdout[-1].shape)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # --- save sequences to text files \n",
    "    task_filepaths_train = saveTasks_to_txt(tasks_list_train, tasklists_train, dset_name, 'train', outdir, experiment_name, scenario='nc')\n",
    "    task_filepaths_train_holdout = saveTasks_to_txt(tasks_list_holdout, tasklists_train, dset_name, 'holdout', outdir, experiment_name, scenario='nc')\n",
    "    task_filepaths_val = saveTasks_to_txt(tasks_list_val, tasklists_train, dset_name, 'validation', outdir, experiment_name, scenario='nc')\n",
    "\n",
    "\n",
    "    return task_filepaths_train, task_filepaths_train_holdout, task_filepaths_val\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveTasks_to_txt(tasklists_subset_indices, tasklists_orig, dset_name, partition, outdir, experiment_name, scenario='nc'):\n",
    "    \n",
    "    # --- save sequences to text files \n",
    "    dest_dir = '%s/%s/%s'%(outdir, dset_name, experiment_name)\n",
    "    utils.makedirectory(dest_dir)\n",
    "    dest_dir = dest_dir + '/%s'%(partition)\n",
    "    utils.makedirectory(dest_dir)\n",
    "\n",
    "    # Create directory for experiment \n",
    "    task_filepaths=[]\n",
    "    for task in range(len(tasklists_orig)):\n",
    "        subset_indices = tasklists_subset_indices[task]\n",
    "        if task<7:\n",
    "            task_filepaths.append(\"%s/%s_%s_task_%d.txt\"%(dest_dir, scenario, partition, task))\n",
    "            write_to_file_subset_8dset(tasklists_orig[task], subset_indices, task_filepaths[-1])\n",
    "        else:\n",
    "            task_filepaths.append(\"%s/%s_%s_task_%d.npy\"%(dest_dir, scenario, partition, task))\n",
    "            np.save(task_filepaths[-1], subset_indices)\n",
    "        \n",
    "    return task_filepaths\n",
    "\n",
    "\n",
    "\n",
    "def write_to_file_subset_8dset(original_txt, subset_indices, new_txt):\n",
    "    with open(original_txt, \"r\") as file_input:\n",
    "        with open(new_txt, \"w\") as output: \n",
    "            for i, line in enumerate(file_input):\n",
    "                if i in subset_indices:\n",
    "                    output.write(line)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dset flowers - Num data 6149 Num labels 102\n",
      "num_samples_task 6149\n",
      "(4306,) (614,) (1229,)\n",
      "Dset scenes - Num data 5359 Num labels 67\n",
      "num_samples_task 5359\n",
      "(3753,) (535,) (1071,)\n",
      "Dset birds - Num data 5994 Num labels 200\n",
      "num_samples_task 5994\n",
      "(4197,) (599,) (1198,)\n",
      "Dset cars - Num data 8144 Num labels 196\n",
      "num_samples_task 8144\n",
      "(5702,) (814,) (1628,)\n",
      "Dset aircraft - Num data 3334 Num labels 70\n",
      "num_samples_task 3334\n",
      "(2335,) (333,) (666,)\n",
      "Dset voc - Num data 1683 Num labels 10\n",
      "num_samples_task 1683\n",
      "(1179,) (168,) (336,)\n",
      "Dset chars - Num data 4623 Num labels 62\n",
      "num_samples_task 4623\n",
      "(3237,) (462,) (924,)\n",
      "Dset svhn - Num data 49191 Num labels 10\n",
      "num_samples_task 49191\n",
      "(34434,) (4919,) (9838,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataroot = '/lab/arios/ProjIntel/incDFM/data/8dset/'\n",
    "\n",
    "experiment_dir = '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL/'\n",
    "\n",
    "\n",
    "task_filepaths_train, task_filepaths_train_holdout, task_filepaths_val = eightdset_Experiments_w_holdout_w_validation_trainset(dataroot, experiment_dir, \\\n",
    "    'Test', dset_name='8dset', holdout_percent=0.2, validation_percent=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_0.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_1.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_2.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_3.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_4.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_5.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_6.txt',\n",
       " '/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//8dset/Test/train/nc_train_task_7.npy']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_filepaths_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class eightdsetTask():\n",
    "    def __init__(self, dataroot, dset_name, split='train', tasklist='task_indices.txt', transform=None, \\\n",
    "        returnIDX=False,  preload=False):\n",
    "        '''\n",
    "        dataroot - for 8dset /lab/arios/ProjIntel/incDFM/data/8dset\n",
    "        dset_name - among the 8 datasets (flowers, aircrafts, birds, cars, char, scenes, voc, svhn)\n",
    "        '''\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.dset_name = dset_name\n",
    "        \n",
    "        self.individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "        \n",
    "        self.order_tasks = {'flowers':0, 'scenes':1, 'birds':2, 'cars':3, 'aircraft':4, 'voc':5,  'chars':6, 'svhn':7}\n",
    "        \n",
    "        self.task_lb = self.order_tasks[self.dset_name]\n",
    "        \n",
    "        if dset_name !='svhn':\n",
    "            self.intern_dset = self.individual_wrappers[dset_name](img_path='%s/%s'%(dataroot, dset_name),\n",
    "                                        txt_path=tasklist,\n",
    "                                        data_transforms=transform)\n",
    "        else:\n",
    "            self.intern_dset = self.individual_wrappers[dset_name]('%s/%s'%(dataroot, dset_name),\n",
    "                                        tasklist=tasklist,\n",
    "                                        data_transforms=transform,\n",
    "                                        split=split)\n",
    "            \n",
    "        self.returnIDX = returnIDX\n",
    "        \n",
    "        self.indices_task_init = np.arange(self.intern_dset.__len__())\n",
    "        \n",
    "        self.indices_task = copy.deepcopy(self.indices_task_init)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.indices_task.shape[0]\n",
    "\n",
    "    def select_random_subset(self, random_num):\n",
    "\n",
    "        inds_keep = np.random.permutation(np.arange(self.indices_task_init.shape[0]))[:random_num]\n",
    "\n",
    "        self.indices_task = self.indices_task_init[inds_keep]\n",
    "        \n",
    "    def select_specific_subset(self, indices_select):\n",
    "        \n",
    "        self.indices_task = self.indices_task_init[indices_select]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.indices_task[idx]\n",
    "\n",
    "        im, class_lbl = self.intern_dset.__getitem__(idx)\n",
    "        \n",
    "        if self.returnIDX:\n",
    "            return im, class_lbl, self.task_lb, idx\n",
    "            \n",
    "        return im, class_lbl, self.task_lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/lab/arios/ProjIntel/incDFM/data/8dset/'\n",
    "class eightdset_normalize():\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "\n",
    "\n",
    "class eightdset_train():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.RandomHorizontalFlip(), TF.ToTensor(), \n",
    "                            #   inaturalist_normalize(),\n",
    "                              ])\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "    \n",
    "transform = eightdset_train()\n",
    "\n",
    "transform_svhn = tforms.svhn_train()    \n",
    "    \n",
    "\n",
    "dset_name='svhn'\n",
    "order_tasks = {'flowers':0, 'scenes':1, 'birds':2, 'cars':3, 'aircraft':4, 'voc':5,  'chars':6, 'svhn':7}\n",
    "tasklist = task_filepaths_train[order_tasks[dset_name]]\n",
    "\n",
    "t_apply = None\n",
    "\n",
    "dataset_8 = eightdsetTask(dataroot, dset_name, split='train', tasklist=tasklist, transform=t_apply, \\\n",
    "        returnIDX=False,  preload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[214., 213., 213.,  ..., 217., 217., 216.],\n",
       "         [215., 214., 211.,  ..., 217., 217., 216.],\n",
       "         [216., 215., 208.,  ..., 217., 217., 216.],\n",
       "         ...,\n",
       "         [179., 193., 208.,  ..., 218., 218., 217.],\n",
       "         [211., 218., 223.,  ..., 217., 218., 217.],\n",
       "         [216., 215., 212.,  ..., 216., 218., 216.]],\n",
       "\n",
       "        [[218., 217., 217.,  ..., 221., 221., 221.],\n",
       "         [216., 215., 216.,  ..., 221., 221., 221.],\n",
       "         [217., 216., 212.,  ..., 221., 221., 221.],\n",
       "         ...,\n",
       "         [183., 195., 209.,  ..., 219., 219., 221.],\n",
       "         [214., 220., 224.,  ..., 218., 219., 221.],\n",
       "         [219., 217., 213.,  ..., 217., 219., 220.]],\n",
       "\n",
       "        [[217., 218., 218.,  ..., 222., 222., 224.],\n",
       "         [219., 218., 216.,  ..., 222., 222., 224.],\n",
       "         [223., 221., 215.,  ..., 222., 222., 224.],\n",
       "         ...,\n",
       "         [188., 200., 213.,  ..., 221., 221., 220.],\n",
       "         [219., 225., 228.,  ..., 220., 221., 220.],\n",
       "         [224., 222., 217.,  ..., 219., 221., 219.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_8.__getitem__(10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6656ba487fb895a60e5d4c5030343c34c763d6fe30d1a6683b717bdb1af8ff9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('incDFM_proj': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

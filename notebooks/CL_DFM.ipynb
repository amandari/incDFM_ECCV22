{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DFM Improvement for CL scenarios\n",
    "\n",
    "Possible areas to tweak:\n",
    "1. Use other layers concatenated (ex early layers or logit) prior to per-class PCA\n",
    "\n",
    "2. Fine-tuning **\n",
    "\n",
    "--------------------------------------------------\n",
    "4. Thresholding + discarding + confidence â€‹+ novelty-rounds\n",
    "\n",
    "5. if we take the scores that are best, are those usually true positives? Most confident predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import colors as mcolors\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import itertools\n",
    "import copy\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "sys.path.append('../src/')\n",
    "import tforms\n",
    "import feature_extraction.feature_extraction_utils as futils\n",
    "from feature_extraction.Network_Latents_Wrapper import NetworkLatents\n",
    "# import classifier as clf\n",
    "import novelty_dfm_CL.novelty_detector as novel\n",
    "import novelty_dfm_CL.novelty_eval as novelval \n",
    "import memory as mem\n",
    "import utils\n",
    "import datasets as dset\n",
    "\n",
    "import datasets_utils as dsetutils\n",
    "\n",
    "\n",
    "import novelty_dfm_CL.classifier as clf\n",
    "import novelty_dfm_CL.novelty_utils as novelu\n",
    "\n",
    "\n",
    "utils.seed_torch(0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters defined in config yaml file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "general_config_path = '/home/amandari/CodeDev/ProjIntel/src/configs/DFM_CL_improvement.yaml' # path to file \n",
    "\n",
    "with open(general_config_path) as fid:\n",
    "        args = Namespace(**yaml.load(fid, Loader=yaml.SafeLoader))\n",
    "\n",
    "torch.set_num_threads(args.num_threads)\n",
    "pin_memory=False\n",
    "\n",
    "# set up save\n",
    "args = utils.config_saving(args)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CL setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "start=time.time()\n",
    "# 1) ----- Dataset \n",
    "if not hasattr(args, 'experiment_filepath'):\n",
    "    args.experiment_filepath = None # have dictionary for defaults???\n",
    "datasets_use = dset.call_dataset(args.dset_name, args.data_dir, args.experiment_dir, experiment_filepath=args.experiment_filepath, experiment_name=args.experiment_name, num_classes_first=args.num_classes_first,\n",
    "                                                type_l_cifar=args.type_l_cifar, num_tasks_cifar=args.num_tasks_cifar, \n",
    "                                                scenario=args.scenario, keep_all_data=args.keep_all_data, scenario_classif=args.scenario_classif, \n",
    "                                                exp_type=args.exp_type, num_per_task=args.num_per_task,\n",
    "                                                shuffle=args.shuffle_order)\n",
    "\n",
    "if args.keep_all_data==True:\n",
    "    train_datasets, train_datasets_new_only, test_datasets, list_tasks, dset_prep = datasets_use\n",
    "else:\n",
    "    train_datasets, test_datasets, list_tasks, dset_prep = datasets_use\n",
    "args.dset_prep = dset_prep\n",
    "if args.num_tasks>0:\n",
    "    num_tasks = args.num_tasks\n",
    "else:\n",
    "    num_tasks = len(train_datasets)\n",
    "test_loaders = [torch.utils.data.DataLoader(test_datasets[t], batch_size=args.batchsize_test,\n",
    "                                                shuffle=True, num_workers=args.num_workers) for t in range(num_tasks)]\n",
    "print('Data set up', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "# 2) ------ Network \n",
    "if len(args.fc_sizes)>0:\n",
    "    fc_sizes = args.fc_sizes.split(\",\")\n",
    "    fc_sizes = [int(x) for x in fc_sizes]\n",
    "else:\n",
    "    fc_sizes = []\n",
    "network = clf.Resnet(dset_prep['total_classes'], resnet_arch=args.net_type, FC_layers=fc_sizes, base_freeze=True)\n",
    "network = network.to(args.device)\n",
    "dfm_inputs = args.dfm_layers_input.split(\",\")\n",
    "dfm_layers_factors = str(args.dfm_layers_factors)\n",
    "dfm_layers_factors = dfm_layers_factors.split(',')\n",
    "dfm_inputs_factors = {}\n",
    "for n in range(len(dfm_inputs)):\n",
    "    dfm_inputs_factors[dfm_inputs[n]]=int(dfm_layers_factors[n]) #adaptive pooling \n",
    "network_inner = NetworkLatents(network, dfm_inputs, pool_factors=dfm_inputs_factors)\n",
    "print('Network set up', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3) ---- Novelty\n",
    "args.detector_params['target_ind']=dset_prep['scenario_classif']\n",
    "args.detector_params['device']=args.device\n",
    "noveltyResults = novelval.save_novelty_results(num_tasks, args.experiment_name_plot, args.dir_save)\n",
    "novelty_detector = novel.NoveltyDetector().create_detector(type=args.novelty_detector_name, params=args.detector_params)\n",
    "print('Novelty Detector set up', time.time()-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4) ---- Memory (if applicable for recomputing DFM per task)\n",
    "# Use raw images in memory\n",
    "coreset = None\n",
    "if args.coreset_size>0:\n",
    "    if args.use_image_as_input:\n",
    "        raw_sizes = {'svhn':32, 'cifar10':32, 'cifar100':32}\n",
    "        input_size = 3*(raw_sizes[args.dset_name]**2)\n",
    "    else:\n",
    "        input_size = network.feat_size  \n",
    "    if args.coreset_size>0 and args.keep_all_data==False:\n",
    "        if args.coreset_size_MB>0:\n",
    "            coreset_size = utils.memory_equivalence(args.coreset_size_MB, input_size, quantizer_dict=None)\n",
    "        else:\n",
    "            coreset_size = args.coreset_size\n",
    "        coreset = mem.CoresetDynamic(coreset_size, target_ind= dset_prep['scenario_classif'], homog_ind=dset_prep['homog_ind'], device=args.device)\n",
    "print('Memory set up', time.time()-start)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "first_task [0, 1] 1 0\n",
      "seq_tasks:  [[0, 1], [2], [3], [4], [5], [6], [7], [8], [9]]\n",
      "*****Prep Data*****\n",
      "prepared filelists 0.4205362796783447\n",
      "None\n",
      "tasks_filepaths_train ['../experiments/cifar10/inc_1class/train/nc_train_task_0.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_1.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_2.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_3.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_4.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_5.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_6.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_7.npy', '../experiments/cifar10/inc_1class/train/nc_train_task_8.npy']\n",
      "number tasks 9\n",
      "prepared datasets 4.001075983047485\n",
      "Data set up 4.00236439704895\n",
      "load contrastive backbone\n",
      "Will fetch activations from:\n",
      "base.8, average pooled by -1\n",
      "Network set up 9.320446491241455\n",
      "Novelty Detector set up 9.32247805595398\n",
      "Memory set up 9.323304176330566\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CL Loop"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "start = time.time()\n",
    "args.patience_lr = int(np.ceil(args.schedule_patience_perepoch*args.num_epochs))\n",
    "\n",
    "per_task_results = {'scores_dist':{}, 'gt':{}}\n",
    "\n",
    "for t in range(num_tasks):\n",
    "    print('###############################')\n",
    "    print('######### task %d ############'%(t))\n",
    "    print('###############################')\n",
    "    current_task = list_tasks[t]\n",
    "\n",
    "\n",
    "    # ------- Set up data \n",
    "    if t>0 and (coreset is not None):\n",
    "        batchsize_new =  int(args.batchsize*0.5)\n",
    "        batchsize_old = args.batchsize - batchsize_new\n",
    "        # --- wrap old data features (latent)\n",
    "        dataset_old = dsetutils.DSET_wrapper_Replay(coreset.coreset_im, coreset.coreset_t, latents=coreset.coreset_latents, transform=args.tf_coreset)\n",
    "        loader_old = torch.utils.data.DataLoader(dataset_old, batch_size=batchsize_old,\n",
    "                                                shuffle=True, num_workers=args.num_workers)\n",
    "    else:\n",
    "        batchsize_new = args.batchsize\n",
    "        batchsize_old = 0\n",
    "        loader_old = None\n",
    "    loader_new = torch.utils.data.DataLoader(train_datasets[t], batch_size=batchsize_new,\n",
    "                                                shuffle=True, num_workers=args.num_workers)\n",
    "    if args.keep_all_data==True:\n",
    "        train_loaders_new_only = torch.utils.data.DataLoader(train_datasets_new_only[t], batch_size=args.batchsize_test,\n",
    "                                                shuffle=True, num_workers=args.num_workers) \n",
    "    else:\n",
    "        train_loaders_new_only = loader_new \n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate Novelty Detector \n",
    "    if t>0:\n",
    "        print('Get scores for novelty detector')\n",
    "        params_score = {'layer':args.dfm_layers_input, 'feature_extractor':network_inner, 'base_apply_score':True, 'target_ind':dset_prep['scenario_classif']}\n",
    "        if args.novelty_detector_name=='odin':\n",
    "            results_novelty = novelval.evaluate_odin(t, novelty_detector, noveltyResults, params_score, train_loaders_new_only, test_loaders)\n",
    "        elif args.novelty_detector_name=='dfm':\n",
    "            results_novelty = novelval.evaluate_dfm(t, novelty_detector, noveltyResults, params_score, train_loaders_new_only, test_loaders)\n",
    "\n",
    "\n",
    "\n",
    "    # call training loop with one liner \n",
    "    args, temp_train_loader = novelu.temporary_loader_novelty(args, loader_new, coreset)\n",
    "\n",
    "\n",
    "    # if training classifier or doing finetuning of backbone \n",
    "    if args.train_clf:\n",
    "        clf.train(t, args, novelty_detector, network_inner, loader_old, loader_new, test_loaders)\n",
    "\n",
    "\n",
    "    print('Generate features for novelty evaluation and coreset')\n",
    "    processed_data = futils.extract_features(network_inner, temp_train_loader, \\\n",
    "        target_ind=dset_prep['scenario_classif'], homog_ind=dset_prep['homog_ind'], \n",
    "        device=args.device, use_raw_images=args.use_image_as_input, raw_image_transform=args.add_tf)\n",
    "\n",
    "\n",
    "    # ----append memory\n",
    "    if t<num_tasks-1:\n",
    "        if args.novelty_detector_name=='dfm':\n",
    "            dfm_x = processed_data[0][args.dfm_layers_input]\n",
    "            dfm_y = processed_data[1]\n",
    "            if args.finetune_backbone=='all':\n",
    "                novelty_detector, dfm_x, dfm_y = novelu.reprocess_data_novelty(args, dfm_x, dfm_y, network_inner, coreset)\n",
    "            novelty_detector.fit_total(dfm_x.T, dfm_y)\n",
    "\n",
    "\n",
    "        # ------ update coreset (if applicable)\n",
    "        if coreset is not None:\n",
    "            print('Append data to memory')\n",
    "            coreset.append_memory(processed_data, current_task)\n",
    "            print('coreset_im', coreset.coreset_im.shape)\n",
    "        \n",
    "    if t>0:\n",
    "        # store results per task \n",
    "        per_task_results['scores_dist'][t] = np.concatenate((results_novelty.new_scores_dist, results_novelty.old_scores_dist), axis=1)\n",
    "\n",
    "        per_task_results['gt'][t] = results_novelty.gt_concat\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "###############################\n",
      "######### task 0 ############\n",
      "###############################\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 2.1776998043060303\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 1000 room_left 0 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 1 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (2, 5000)\n",
      "scores_dist (2, 2000)\n",
      "old_scores_dist (2, 2000)\n",
      "New task 1, AUROC = 0.956, AUPR = 0.981, AUPR_NORM = 0.981\n",
      "new_scores [1.17800283 1.31865442 0.45209706 ... 0.44282067 0.60812485 1.04051352]\n",
      "old_scores [0.09962666 0.10178394 0.11242108 ... 0.26372415 0.16067803 0.20280236]\n",
      "DFM Results -  Auroc 0.956, Aupr 0.981, Aupr_norm 0.981\n",
      "Average Accuracy per class old 0.9810\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.1354477405548096\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 333 room_left 667 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 2 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (3, 5000)\n",
      "scores_dist (3, 2000)\n",
      "scores_dist (3, 1000)\n",
      "old_scores_dist (3, 3000)\n",
      "New task 2, AUROC = 0.918, AUPR = 0.943, AUPR_NORM = 0.943\n",
      "new_scores [0.44539306 0.77547729 0.34196824 ... 0.26714206 0.2544049  0.30853379]\n",
      "old_scores [0.26880687 0.18673703 0.13171919 ... 0.19214225 0.17926642 0.30564374]\n",
      "DFM Results -  Auroc 0.918, Aupr 0.943, Aupr_norm 0.943\n",
      "Average Accuracy per class old 0.9573\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.145890235900879\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 250 room_left 750 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 3 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (4, 5000)\n",
      "scores_dist (4, 2000)\n",
      "scores_dist (4, 1000)\n",
      "scores_dist (4, 1000)\n",
      "old_scores_dist (4, 4000)\n",
      "New task 3, AUROC = 0.651, AUPR = 0.662, AUPR_NORM = 0.662\n",
      "new_scores [0.30673063 0.16710402 0.32503542 ... 0.31376353 0.19313487 0.22746545]\n",
      "old_scores [0.39667997 0.15087998 0.17197469 ... 0.38241535 0.33791906 0.17039794]\n",
      "DFM Results -  Auroc 0.651, Aupr 0.662, Aupr_norm 0.662\n",
      "Average Accuracy per class old 0.9280\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.1302313804626465\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 200 room_left 800 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 4 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (5, 5000)\n",
      "scores_dist (5, 2000)\n",
      "scores_dist (5, 1000)\n",
      "scores_dist (5, 1000)\n",
      "scores_dist (5, 1000)\n",
      "old_scores_dist (5, 5000)\n",
      "New task 4, AUROC = 0.800, AUPR = 0.780, AUPR_NORM = 0.780\n",
      "new_scores [0.31651643 0.30483273 0.21130356 ... 0.55158341 0.22273293 0.39385715]\n",
      "old_scores [0.17987472 0.25350472 0.20926961 ... 0.44502327 0.30067515 0.28055578]\n",
      "DFM Results -  Auroc 0.800, Aupr 0.780, Aupr_norm 0.780\n",
      "Average Accuracy per class old 0.8940\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.1169521808624268\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 166 room_left 834 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 5 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (6, 5000)\n",
      "scores_dist (6, 2000)\n",
      "scores_dist (6, 1000)\n",
      "scores_dist (6, 1000)\n",
      "scores_dist (6, 1000)\n",
      "scores_dist (6, 1000)\n",
      "old_scores_dist (6, 6000)\n",
      "New task 5, AUROC = 0.624, AUPR = 0.549, AUPR_NORM = 0.595\n",
      "new_scores [0.3080759  0.26938725 0.2313938  ... 0.28230318 0.22748674 0.28764388]\n",
      "old_scores [0.30375379 0.29430073 0.15556535 ... 0.20782295 0.21980655 0.25811964]\n",
      "DFM Results -  Auroc 0.624, Aupr 0.549, Aupr_norm 0.593\n",
      "Average Accuracy per class old 0.8513\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.0780746936798096\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 142 room_left 858 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 6 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (7, 5000)\n",
      "scores_dist (7, 2000)\n",
      "scores_dist (7, 1000)\n",
      "scores_dist (7, 1000)\n",
      "scores_dist (7, 1000)\n",
      "scores_dist (7, 1000)\n",
      "scores_dist (7, 1000)\n",
      "old_scores_dist (7, 7000)\n",
      "New task 6, AUROC = 0.756, AUPR = 0.633, AUPR_NORM = 0.702\n",
      "new_scores [0.18583319 0.25940511 0.27757215 ... 0.33198354 0.17460558 0.21665467]\n",
      "old_scores [0.0928371  0.20700577 0.15174705 ... 0.2681711  0.15897878 0.15435815]\n",
      "DFM Results -  Auroc 0.756, Aupr 0.633, Aupr_norm 0.706\n",
      "Average Accuracy per class old 0.8371\n",
      "Generate features for novelty evaluation and coreset\n",
      "n_comp var 0.995\n",
      "end fit 1.1471047401428223\n",
      "Append data to memory\n",
      "*******Appending to CORESET******\n",
      "total_size 1000 room_new 125 room_left 875 coreset now 1000\n",
      "coreset_im torch.Size([1000, 2048])\n",
      "###############################\n",
      "######### task 7 ############\n",
      "###############################\n",
      "Get scores for novelty detector\n",
      "scores_dist (8, 5000)\n",
      "scores_dist (8, 2000)\n",
      "scores_dist (8, 1000)\n",
      "scores_dist (8, 1000)\n",
      "scores_dist (8, 1000)\n",
      "scores_dist (8, 1000)\n",
      "scores_dist (8, 1000)\n",
      "scores_dist (8, 1000)\n",
      "old_scores_dist (8, 8000)\n",
      "New task 7, AUROC = 0.792, AUPR = 0.661, AUPR_NORM = 0.756\n",
      "new_scores [0.40356997 0.5103581  0.40539324 ... 0.25678316 0.35979038 0.21543916]\n",
      "old_scores [0.26149914 0.30423784 0.21704507 ... 0.13055973 0.24494186 0.17731753]\n",
      "DFM Results -  Auroc 0.792, Aupr 0.661, Aupr_norm 0.755\n",
      "Average Accuracy per class old 0.8274\n",
      "Generate features for novelty evaluation and coreset\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save Dist Scores + Ground Truth "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "with open('%s/results_tasks.pickle'%(args.dir_save), 'wb') as handle:\n",
    "    pickle.dump(per_task_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['scores_dist', 'gt'])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('remind_proj': conda)"
  },
  "interpreter": {
   "hash": "05b150e87e16a7f15e68a52cad20b8449e4511c9ed1c6048915793505d6c322d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load contrastive backbone\n",
      "Will fetch activations from:\n",
      "base.8, average pooled by -1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import itertools\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "sys.path.append('../src')\n",
    "import tforms\n",
    "import feature_extraction.feature_extraction_utils as futils\n",
    "from feature_extraction.Network_Latents_Wrapper import NetworkLatents\n",
    "\n",
    "import datasets_utils as dsetutils\n",
    "import utils\n",
    "\n",
    "sys.path.append('../src/novelty_dfm_CL')\n",
    "\n",
    "import novelty_dfm_CL.datasets_holdout_validation as dseth\n",
    "import novelty_dfm_CL.novelty_detector as novel\n",
    "import novelty_dfm_CL.novelty_eval as novelval \n",
    "import novelty_dfm_CL.classifier as clf\n",
    "import novelty_dfm_CL.novelty_utils as novelu\n",
    "import novelty_dfm_CL.incDFM_w_validation as novelinc\n",
    "import novelty_dfm_CL.novelty_dataset_wrappers as dwrap\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "# parameters\n",
    "\n",
    "device = 0\n",
    "ood_method='odin'\n",
    "dset_id_name = 'cifar100'\n",
    "dset_od_name = 'svhn'\n",
    "extractor_name = 'resnet50_contrastive'\n",
    "finetune_backbone=False\n",
    "\n",
    "\n",
    "num_outputs_map = {'cifar10':10, 'svhn':10, 'cifar100':100}\n",
    "\n",
    "\n",
    "num_classes_id = num_outputs_map[dset_id_name]\n",
    "num_classes_od = num_outputs_map[dset_od_name]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "dir_save='/lab/arios/ProjIntel/incDFM/sandbox/results_ood_sup_2d/OOD_%s_ID_%s_OD_%s/'%(ood_method, dset_id_name, dset_od_name)\n",
    "utils.makedirectory(dir_save)\n",
    "\n",
    "def get_features(dataset, network_inner, device=0):\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=100,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "    start = time.time()\n",
    "    print('feat extraction begin')\n",
    "    current_features = futils.extract_features(network_inner, loader, \\\n",
    "            target_ind=1, homog_ind=-2, device=device)\n",
    "\n",
    "    print('feat extraction done', time.time()-start)\n",
    "    feat_name = 'base.8'\n",
    "\n",
    "    X = current_features[0][feat_name]\n",
    "    Y = current_features[-2]\n",
    "\n",
    "    return X, Y, current_features\n",
    "\n",
    "\n",
    "\n",
    "## Include validation set \n",
    "network = clf.Resnet(num_classes_id, resnet_arch=extractor_name, FC_layers=[],  \n",
    "            resnet_base=-1, multihead_type='single', base_freeze=True, pretrained_weights=None)\n",
    "network = network.to(device)\n",
    "# run extractor \n",
    "network_inner = NetworkLatents(network, ['base.8'], pool_factors={'base.8':-1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ID and OD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_tasks [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38, 39: 39, 40: 40, 41: 41, 42: 42, 43: 43, 44: 44, 45: 45, 46: 46, 47: 47, 48: 48, 49: 49, 50: 50, 51: 51, 52: 52, 53: 53, 54: 54, 55: 55, 56: 56, 57: 57, 58: 58, 59: 59, 60: 60, 61: 61, 62: 62, 63: 63, 64: 64, 65: 65, 66: 66, 67: 67, 68: 68, 69: 69, 70: 70, 71: 71, 72: 72, 73: 73, 74: 74, 75: 75, 76: 76, 77: 77, 78: 78, 79: 79, 80: 80, 81: 81, 82: 82, 83: 83, 84: 84, 85: 85, 86: 86, 87: 87, 88: 88, 89: 89, 90: 90, 91: 91, 92: 92, 93: 93, 94: 94, 95: 95, 96: 96, 97: 97, 98: 98, 99: 99}\n",
      "seq_tasks_targets [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
      "seq_tasks:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] 1\n",
      "*****Prep Data*****\n",
      "Create New Experiment\n",
      "num_samples_task 5000\n",
      "/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//cifar100/MT_Supplementary/\n",
      "number tasks 1\n",
      "prepared datasets 2.2172954082489014\n",
      "5 4495 500 1000\n",
      "first_task 10 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 10 0\n",
      "seq_tasks:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] 1\n",
      "*****Prep Data*****\n",
      "Create New Experiment\n",
      "num_samples_task 73257\n",
      "/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL//svhn/MT_Supplementary/\n",
      "number tasks 1\n",
      "prepared datasets 11.34778618812561\n",
      "73 65859 7325 26032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "experiment_name='MT_Supplementary'\n",
    "data_dir = '/lab/arios/ProjIntel/incDFM/data/'\n",
    "experiment_dir='/lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Experiments_DFM_CL/'\n",
    "\n",
    "holdout_percent=0.001\n",
    "val_percent=0.1\n",
    "\n",
    "\n",
    "data_ID = dseth.call_dataset_holdout_w_validation(dset_id_name, data_dir, experiment_dir, \n",
    "                                        experiment_filepath=None, experiment_name=experiment_name, \n",
    "                                        holdout_percent=holdout_percent,  val_holdout=val_percent, scenario='nc', \n",
    "                                        num_per_task=num_classes_id, num_classes_first=num_classes_id, \n",
    "                                        num_tasks=1, \n",
    "                                        shuffle=False, preload=False, keep_all_data=False, \\\n",
    "                                            equalize_labels=False, clip_labels=False, clip_max=15000)\n",
    "\n",
    "\n",
    "train_dataset_ID, train_holdout_dataset_ID, val_dataset_ID, test_dataset_ID, list_tasks_ID, list_tasks_targets_ID, dset_prep_ID = data_ID\n",
    "print(train_holdout_dataset_ID[0].__len__(), train_dataset_ID[0].__len__(), val_dataset_ID[0].__len__(), test_dataset_ID[0].__len__())\n",
    "train_dataset_ID, train_holdout_dataset_ID, val_dataset_ID, test_dataset_ID = train_dataset_ID[0], train_holdout_dataset_ID[0], val_dataset_ID[0], test_dataset_ID[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_OD = dseth.call_dataset_holdout_w_validation(dset_od_name, data_dir, experiment_dir, \n",
    "                                        experiment_filepath=None, experiment_name=experiment_name, \n",
    "                                        holdout_percent=holdout_percent,  val_holdout=val_percent, scenario='nc', \n",
    "                                        num_per_task=num_classes_od, num_classes_first=num_classes_od, \n",
    "                                        num_tasks=1, \n",
    "                                        shuffle=False, preload=False, keep_all_data=False, \\\n",
    "                                            equalize_labels=False, clip_labels=False, clip_max=15000)\n",
    "\n",
    "train_dataset_OD, train_holdout_dataset_OD, val_dataset_OD, test_dataset_OD, list_tasks_OD, list_tasks_targets_OD, dset_prep_OD = data_OD\n",
    "\n",
    "print(train_holdout_dataset_OD[0].__len__(), train_dataset_OD[0].__len__(), val_dataset_OD[0].__len__(), test_dataset_OD[0].__len__())\n",
    "\n",
    "train_dataset_OD, train_holdout_dataset_OD, val_dataset_OD, test_dataset_OD = train_dataset_OD[0], train_holdout_dataset_OD[0], val_dataset_OD[0], test_dataset_OD[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get scores for novelty detector\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "name_experiment='OOD_only2_ID_%s_OD_%s'%(dset_id_name,dset_od_name)\n",
    "\n",
    "\n",
    "ood_config_paths = {'dfm':'/lab/arios/ProjIntel/incDFM/src/configs_new/ood_method/dfm_ood_config.yaml',\n",
    "                    'incdfm':'/lab/arios/ProjIntel/incDFM/src/configs_new/ood_method/incdfm_ood_config.yaml',\n",
    "                    'mahal':'/lab/arios/ProjIntel/incDFM/src/configs_new/ood_method/mahal_ood_config.yaml',\n",
    "                    'odin':'/lab/arios/ProjIntel/incDFM/src/configs_new/ood_method/odin_ood_config.yaml',\n",
    "                    'softmax':'/lab/arios/ProjIntel/incDFM/src/configs_new/ood_method/softmax_ood_config.yaml',\n",
    "}\n",
    "\n",
    "ood_config = ood_config_paths[ood_method]\n",
    "\n",
    "\n",
    "\n",
    "with open(ood_config) as fid:\n",
    "    params = Namespace(**yaml.load(fid, Loader=yaml.SafeLoader))\n",
    "        \n",
    "\n",
    "# Set up detector params \n",
    "params.detector_params['target_ind']=1\n",
    "params.detector_params['device']=device\n",
    "params.detector_params['num_classes']=num_classes_id\n",
    "if ood_method=='odin':\n",
    "    params.detector_params['base_network']=network #simple network (not wrapper) - Is this problematic? TODO\n",
    "    params.detector_params['num_classes_fine_tasks']=10\n",
    "    params.detector_params['criterion']= nn.CrossEntropyLoss()\n",
    "    params.detector_params['num_epochs']=20\n",
    "    params.detector_params['train_technique']=1\n",
    "    params.detector_params['lr']=0.001\n",
    "    params.detector_params['patience_lr'] = 0.25\n",
    "    params.detector_params['schedule_decay'] = 0.5\n",
    "    params.detector_params['step_size_epoch_lr']= 7\n",
    "    params.detector_params['gamma_step_lr']= 0.1\n",
    "elif ood_method=='softmax':\n",
    "    params.detector_params['base_network']=network #simple network (not wrapper) - Is this problematic? TODO\n",
    "    params.detector_params['num_classes_fine_tasks']=10\n",
    "    \n",
    "    \n",
    "\n",
    "print('Get scores for novelty detector')\n",
    "noveltyResults = novelval.save_novelty_results(1, name_experiment, dir_save)\n",
    "# train_loaders_new_only evaluates only current new data (unseen by classifier/main model). \n",
    "params_score = {'layer':'base.8', 'feature_extractor':network_inner, 'base_apply_score':True, 'target_ind':1}\n",
    "# print('new', next(iter(train_loaders_new_only[t])))\n",
    "\n",
    "novelty_detector = novel.NoveltyDetector().create_detector(type=ood_method, params=params.detector_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting odin on cifar100\n",
      "##########epoch 0###########\n",
      "Number batches:  45\n",
      "Train Epoch: 0 [0/45 (0.000)]\tLoss: 2.2954\tAcc_Train_new:0.090\n",
      "Train Epoch: 0 [10/45 (22.222)]\tLoss: 2.1805\tAcc_Train_new:0.150\n",
      "Train Epoch: 0 [20/45 (44.444)]\tLoss: 2.1116\tAcc_Train_new:0.220\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#Test loader\n",
    "val_loader_ID = torch.utils.data.DataLoader(val_dataset_ID,\\\n",
    "            batch_size=100, shuffle=True, num_workers=4)\n",
    "\n",
    "print('Fitting %s on %s'%(ood_method, dset_id_name))\n",
    "\n",
    "if ood_method=='dfm' or ood_method=='mahal' or ood_method=='incdfm':\n",
    "    print('get features ID - %s'%dset_id_name)\n",
    "    # Training - Only ID -------------\n",
    "    Feats_ID_train = get_features(train_dataset_ID, network_inner, device=device)\n",
    "    novelty_detector.fit_total(Feats_ID_train[0].T, Feats_ID_train[1])\n",
    "else:\n",
    "    from novelty_dfm_CL.train_main import train_main_epoch\n",
    "    from novelty_dfm_CL.test_main import test_main\n",
    "    # train Loader\n",
    "    train_loader_ID = torch.utils.data.DataLoader(dwrap.NovelTask(0, num_classes_id, train_dataset_ID, use_coarse=False), \\\n",
    "                batch_size=100, shuffle=True, num_workers=4)\n",
    "    \n",
    "    # train classifier jointly \n",
    "    if finetune_backbone==True:\n",
    "        network_inner.model.base.train(True)\n",
    "        network_inner.base_freeze = False\n",
    "        for param in network_inner.model.base.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        network_inner.model.base.train(False)\n",
    "        network_inner.base_freeze = True\n",
    "        for param in network_inner.model.base.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    if ood_method=='odin':\n",
    "        clf_parameters = []\n",
    "        for name, parameter in network_inner.model.named_parameters():\n",
    "            if name == 'h.h.weight' or name == 'h.h.bias':\n",
    "                pass\n",
    "            else:\n",
    "                clf_parameters.append(parameter)\n",
    "    else:\n",
    "        clf_parameters = network_inner.model.parameters()\n",
    "\n",
    "        \n",
    "    optimizer_main = optim.Adam(filter(lambda p: p.requires_grad, clf_parameters), lr=0.001)\n",
    "    scheduler_main = optim.lr_scheduler.ReduceLROnPlateau(optimizer_main, 'min', patience=0.25, factor=0.5, min_lr=0.00001)\n",
    "    \n",
    "    # ----- relative batchsizes between old and new \n",
    "    for epoch in range(10):\n",
    "        print('##########epoch %d###########'%epoch)\n",
    "        network_inner.model.train()\n",
    "        if finetune_backbone==True:\n",
    "            network_inner.model.base.train(True)\n",
    "            network_inner.base_freeze = False\n",
    "            for param in network_inner.model.base.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            network_inner.model.base.train(False)\n",
    "            network_inner.base_freeze = True\n",
    "            for param in network_inner.model.base.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        train_main_epoch(epoch, train_loader_ID, None, network_inner, optimizer_main,  device=device, OOD_class=novelty_detector,\\\n",
    "                    feature_name='base.8', weight_old=0.5, cut_epoch_short=False,\\\n",
    "                    quantizer=None, target_ind=1, cuda=True, display_freq=10)\n",
    "\n",
    "        val_loss = test_main(epoch, 0, [val_loader_ID], network_inner, novelty_detector, dir_save, \\\n",
    "            feature_name='base.8', target_ind=1, \\\n",
    "                quantizer=None, cuda=True, device=device)\n",
    "\n",
    "        # Update schedulers \n",
    "        scheduler_main.step(val_loss)\n",
    "\n",
    "\n",
    "        if novelty_detector.name == 'odin':\n",
    "            novelty_detector.h_scheduler.step(val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_dset.__len__() 25950\n"
     ]
    }
   ],
   "source": [
    "current_dset = dwrap.CurrentTask(test_dataset_OD, [test_dataset_ID])\n",
    "current_loader = torch.utils.data.DataLoader(current_dset, batch_size=100,\n",
    "                                                shuffle=False, num_workers=4)\n",
    "\n",
    "print('current_dset.__len__()', current_dset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Scores for ID svhn using odin\n",
      "********Score func: h *************\n",
      "Noise magnitude 0.00250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.12705427 -0.09504923 -0.14548695 ... -0.22997114 -0.33823946\n",
      " -0.33759266]\n",
      "AUROC: 0.8332665266457681 aupr 0.836433093078297 aupr_norm 0.836433093078297\n",
      "Noise magnitude 0.00500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.13559569 -0.10476782 -0.15349543 ... -0.23812331 -0.34583488\n",
      " -0.34601322]\n",
      "AUROC: 0.8359275172413793 aupr 0.8389812704875125 aupr_norm 0.8389812704875125\n",
      "Noise magnitude 0.01000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.15260047 -0.1241117  -0.16945082 ... -0.25430274 -0.36086839\n",
      " -0.36264735]\n",
      "AUROC: 0.8410483448275861 aupr 0.8440008971196538 aupr_norm 0.8440008971196538\n",
      "Noise magnitude 0.02000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.18616922 -0.16224048 -0.20100628 ... -0.28604779 -0.39020824\n",
      " -0.39495802]\n",
      "AUROC: 0.8504247460815046 aupr 0.8539970943606201 aupr_norm 0.8539970943606201\n",
      "Noise magnitude 0.04000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.25059289 -0.23491056 -0.26187825 ... -0.34625694 -0.44528419\n",
      " -0.45485133]\n",
      "AUROC: 0.8655822978056426 aupr 0.8720779775366894 aupr_norm 0.8720779775366894\n",
      "Noise magnitude 0.08000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Scores | Processing image batch 260/260: 100%|██████████| 260/260 [00:44<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores [-0.36288959 -0.35838595 -0.3694981  ... -0.44886035 -0.53721809\n",
      " -0.55136108]\n",
      "AUROC: 0.8842161285266459 aupr 0.896161028033454 aupr_norm 0.896161028033454\n",
      "New task 1, AUROC = 0.884, AUPR = 0.896, AUPR_NORM = 0.896\n",
      "new_scores [-0.36288959 -0.35838595 -0.3694981  ... -0.3146278  -0.36126465\n",
      " -0.35164791]\n",
      "old_scores [-0.3900716  -0.52445227 -0.61952615 ... -0.44886035 -0.53721809\n",
      " -0.55136108]\n",
      "best scores [-0.36288959 -0.35838595 -0.3694981  ... -0.44886035 -0.53721809\n",
      " -0.55136108]\n",
      "ODIN Best Values [Score_func h -Noise 0.080]--> Best overall: Auroc 0.884, Aupr 0.896, Aupr_norm 0.896\n"
     ]
    }
   ],
   "source": [
    "args=Namespace()\n",
    "params_score['device']=device\n",
    "print('Computing Scores for ID %s using %s'%(dset_od_name, ood_method))\n",
    "if ood_method=='dfm' or ood_method=='odin' or ood_method=='softmax' or ood_method=='mahal':\n",
    "    \n",
    "    # 2) Threshold Novel/Old - Binary prediction \n",
    "    if ood_method=='odin':\n",
    "        results_novelty = novelval.evaluate_odin_CL(1, novelty_detector, noveltyResults, params_score, current_loader)\n",
    "        # invert scores for ODIN so that high scores are novelty (1)\n",
    "    else:\n",
    "        results_novelty = novelval.evaluate_simple_CL(1, novelty_detector, noveltyResults, params_score, current_loader, ood_method)\n",
    "\n",
    "else:\n",
    "    args.params_score = Namespace(**params_score)\n",
    "    args.num_samples = current_dset.__len__()\n",
    "    args.w = 0.5\n",
    "    args.alg_dfm='simple'\n",
    "    args.max_iter_pseudolabel=10\n",
    "    args.threshold_percentile=85\n",
    "    current_old_new_ratio = 1.0\n",
    "    args.validation_iid_loader = val_loader_ID\n",
    "    args.percentile_val_threshold=95\n",
    "    args.batchsize_test=100\n",
    "    args.num_workers=4\n",
    "    args.novelty_detector_name=ood_method\n",
    "    args.detector_params = params.detector_params\n",
    "    args.dset_prep = dset_prep_ID\n",
    "    args.device=device\n",
    "    args.use_image_as_input = False\n",
    "    args.add_tf=None\n",
    "    args.dfm_layers_input = 'base.8'\n",
    "    # Threshold_n_Select_Iters(params)\n",
    "    if args.alg_dfm == 'simple':\n",
    "        SelectTh = novelinc.Threshold_n_Select_Iters(args)\n",
    "    # elif args.alg_dfm == 'tug':\n",
    "    #     SelectTh = novelinc.Threshold_Tug_incDFM(args)\n",
    "        \n",
    "    inds_pred_novel,_ = SelectTh.select_novel(1, current_dset, novelty_detector, noveltyResults)\n",
    "    results_novelty = SelectTh.evaluate_CL(1, current_dset, novelty_detector, noveltyResults)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6656ba487fb895a60e5d4c5030343c34c763d6fe30d1a6683b717bdb1af8ff9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('incDFM_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

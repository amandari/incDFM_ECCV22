{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import colors as mcolors\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "\n",
    "def compute_stats_per_task(t, results):\n",
    "\n",
    "    gt_novelty = results.gt_novelty[t]\n",
    "\n",
    "    inds_above = results.preds[t].astype(bool)\n",
    "\n",
    "    # precision_novelty = gt_novelty[inds_above].sum()/gt_novelty[inds_above].shape[0]\n",
    "    recall_novelty = gt_novelty[inds_above].sum()/gt_novelty.sum()\n",
    "\n",
    "    inds_new = (gt_novelty==1)\n",
    "\n",
    "    accuracy_overall = sum(inds_new == inds_above)/inds_new.shape[0]\n",
    "\n",
    "    precision_novelty = np.sum(gt_novelty[inds_above]==np.ones(gt_novelty[inds_above].shape[0]))/gt_novelty[inds_above].shape[0]\n",
    "\n",
    "\n",
    "    # Compute F-scores \n",
    "    labels = gt_novelty\n",
    "    predictions = np.zeros(gt_novelty.shape)\n",
    "    predictions[inds_above]=1\n",
    "    f_1 = sklearn.metrics.fbeta_score(labels, predictions, beta=1.0)\n",
    "    f_half = sklearn.metrics.fbeta_score(labels, predictions, beta=0.5)\n",
    "    f_2 = sklearn.metrics.fbeta_score(labels, predictions, beta=2.0)\n",
    "\n",
    "    return (precision_novelty, recall_novelty, f_half, f_1, f_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance (F1,Fbeta, etc) x Tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='f_1'\n",
    "dataset = 'cifar10'\n",
    "exp_type='1class'\n",
    "num_tasks=8\n",
    "\n",
    "results_dir = '../src/novelty_dfm_CL/Results_DFM_CL/%s/'%dataset\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "versions = []\n",
    "names  = ['incDFM (Ours)', 'DFM', 'Mahalanobis', 'Generalized Odin', 'Softmax']\n",
    "lines = ['-' for i in range(len(names))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_colors = len(names)\n",
    "\n",
    "colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(number_of_colors)])\n",
    "             for i in range(number_of_colors)]\n",
    "\n",
    "plot_order={ 'Precision':0, 'Recall':1, 'f_0.5':2, 'f_1': 3, 'f_2': 4}\n",
    "num_tasks_map = {'cifar10':8, 'emnist':24, 'svhn':8, 'inaturalist21':7, 'cifar100':18, '8dsets':6}\n",
    "\n",
    "num_tasks = num_tasks_map[dataset]\n",
    "\n",
    "\n",
    "# Issue: in IterTh version, one does not select via a single threshold. Would have needed to save the indices selected, or the predictions \n",
    "\n",
    "\n",
    "scores = []\n",
    "tasks = []\n",
    "plt.rcParams['figure.dpi'] = 200 # default for me was 75\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.title('%s through continual tasks for %s'%(metric.upper(), dataset))\n",
    "\n",
    "for n in range(len(versions)):\n",
    "\n",
    "    scores.append([])\n",
    "    \n",
    "    with open('%s/%s/results_tasks.pickle'%(results_dir, versions[n]), 'rb') as handle:\n",
    "        per_task_results_comp = pickle.load(handle)\n",
    "        \n",
    "    if isinstance(per_task_results_comp, dict):\n",
    "        per_task_results_comp = Namespace(**per_task_results_comp)\n",
    "    \n",
    "    for t in np.arange(1,num_tasks+1):\n",
    "        vals = compute_stats_per_task(t, per_task_results_comp)\n",
    "        scores[n].append(vals[plot_order[metric]])\n",
    "\n",
    "\n",
    "    plt.plot(np.arange(1,len(scores[n])+1), scores[n], marker='.', linestyle=lines[n], label=names[n], color=colors[n])\n",
    "\n",
    "\n",
    "# plt.tick_params(top='off',  right='off')\n",
    "\n",
    "plt.ylim([0.3,1.02])\n",
    "# plt.xlim([-0.01+1,0.01 +len(tasks)-1])\n",
    "plt.xlabel('Tasks')\n",
    "plt.ylabel(metric)\n",
    "plt.legend(fontsize='large', loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.savefig('%s/Scores_tasks_time.png'%dir_save)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6656ba487fb895a60e5d4c5030343c34c763d6fe30d1a6683b717bdb1af8ff9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('incDFM_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from matplotlib import image\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('../src/')\n",
    "import tforms\n",
    "import feature_extraction.feature_extraction_utils as futils\n",
    "from feature_extraction.Network_Latents_Wrapper import NetworkLatents\n",
    "# import classifier as clf\n",
    "\n",
    "import novelty_ODD.ODIN_utils as odutils\n",
    "import novelty_dfm_CL.novelty_detector as novel\n",
    "import novelty_dfm_CL.novelty_eval as novelval \n",
    "import novelty_dfm_CL.classifier as clf\n",
    "import novelty_dfm_CL.novelty_utils as novelu\n",
    "\n",
    "\n",
    "import memory as mem\n",
    "import utils\n",
    "import datasets as dset\n",
    "import novelty_dfm_CL.datasets_holdout as dseth\n",
    "import datasets_utils as dsetutils\n",
    "\n",
    "import novelty_dfm_CL.scoring_multi_threshold as ThScores\n",
    "\n",
    "\n",
    "from novelty_dfm_CL.novelty_eval import acuracy_report, scores_metrics_results\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from tqdm import tqdm \n",
    "\n",
    "utils.seed_torch(0)\n",
    "\n",
    "device = 2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "scores = np.array([-2,-2,-3,-0.1, -0.2])\n",
    "trues = np.array([0,1,1,1,1])\n",
    "# trues = np.logical_not(trues).astype(int)\n",
    "\n",
    "roc_auc_score(trues, scores), np.percentile(scores, 80), scores>-0.18"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.625, -0.18, array([False, False, False,  True, False]))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "h_dict = {\n",
    "    'cosine':   odutils.CosineDeconf,\n",
    "    'inner':    odutils.InnerDeconf,\n",
    "    'baseline': odutils.InnerDeconf,\n",
    "    'euclid':   odutils.EuclideanDeconf\n",
    "}\n",
    "\n",
    "# for fake out of distribution \n",
    "generating_loaders_dict = {\n",
    "    'Gaussian': odutils.GaussianLoader,\n",
    "    'Uniform': odutils.UniformLoader\n",
    "}\n",
    "# for cifar10\n",
    "r_mean = 125.3/255\n",
    "g_mean = 123.0/255\n",
    "b_mean = 113.9/255\n",
    "r_std = 63.0/255\n",
    "g_std = 62.1/255\n",
    "b_std = 66.7/255\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding = 4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((r_mean, g_mean, b_mean), (r_std, g_std, b_std)),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.CenterCrop((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((r_mean, g_mean, b_mean), (r_std, g_std, b_std)),\n",
    "])\n",
    "\n",
    "\n",
    "def get_datasets(data_dir, data_name, batch_size):\n",
    "\n",
    "    train_set_in = torchvision.datasets.CIFAR10(root=f'{data_dir}/cifar10', train=True, download=True, transform=train_transform)\n",
    "    test_set_in  = torchvision.datasets.CIFAR10(root=f'{data_dir}/cifar10', train=False, download=True, transform=test_transform)\n",
    "    \n",
    "    if data_name == 'Gaussian' or data_name == 'Uniform':\n",
    "        normalizer = odutils.Normalizer(r_mean, g_mean, b_mean, r_std, g_std, b_std)\n",
    "        outlier_loader = generating_loaders_dict[data_name](batch_size = batch_size, num_batches = int(10000 / batch_size), transformers = [normalizer])\n",
    "    elif data_name=='SVHN':\n",
    "        outlier_set = torchvision.datasets.SVHN(root=f'{data_dir}/svhn', split='train', transform=train_transform, download=True)\n",
    "        outlier_loader = DataLoader(outlier_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    else:\n",
    "        outlier_set  = getattr(torchvision.datasets, data_name.upper())(f'{data_dir}/{data_name}', transform=test_transform)\n",
    "        outlier_loader       =  DataLoader(outlier_set,       batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    test_indices      = list(range(len(test_set_in)))\n",
    "    validation_set_in = Subset(test_set_in, test_indices[:1000])\n",
    "    test_set_in       = Subset(test_set_in, test_indices[1000:])\n",
    "\n",
    "    train_loader_in      =  DataLoader(train_set_in,      batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "    validation_loader_in =  DataLoader(validation_set_in, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader_in       =  DataLoader(test_set_in,       batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader_in, validation_loader_in, test_loader_in, outlier_loader\n",
    "\n",
    "\n",
    "\n",
    "def calc_tnr(id_test_results, ood_test_results, tpr_threshold=0.95):\n",
    "    scores = np.concatenate((id_test_results, ood_test_results))\n",
    "    trues = np.array(([1] * len(id_test_results)) + ([0] * len(ood_test_results)))\n",
    "    fpr, tpr, thresholds = roc_curve(trues, scores)\n",
    "    return 1 - fpr[np.argmax(tpr>=tpr_threshold)]\n",
    "\n",
    "def calc_auroc(id_test_results, ood_test_results):\n",
    "    #calculate the AUROC\n",
    "    scores = np.concatenate((id_test_results, ood_test_results))\n",
    "    print(scores)\n",
    "    trues = np.array(([1] * len(id_test_results)) + ([0] * len(ood_test_results)))\n",
    "    result = roc_auc_score(trues, scores)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "data_dir = '../../data/'\n",
    "data_name_out = 'SVHN'\n",
    "batch_size = 100\n",
    "#get outlier data\n",
    "train_data, val_data, test_data, open_data = get_datasets(data_dir, data_name_out, batch_size)\n",
    "\n",
    "\n",
    "# train_datasets, train_holdout_datasets, test_datasets, seq_tasks, dset_prep = dset.call_dataset_holdout('cifar10', data_dir, './experiments', experiment_filepath=None, \n",
    "#                                         experiment_name='tryout2', holdout_percent=0.25,  max_holdout=0.75, scenario='nc', \n",
    "#                                         scenario_classif='class', exp_type='class', num_per_task=1, num_classes_first=2, \n",
    "#                                         shuffle=False, preload=False)\n",
    "\n",
    "# train_data      =  DataLoader(train_datasets[0],      batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "# val_data =  DataLoader(train_holdout_datasets[0], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# test_data = DataLoader(test_datasets[0], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "# open_data       =  DataLoader(train_datasets[1],       batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ../../data//svhn/train_32x32.mat\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dde872a42b7440bbdc226816ac5a3e7"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ODIN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "freeze=False\n",
    "resnet_arch='resnet18'\n",
    "fc_sizes = [4096]\n",
    "device = 0\n",
    "\n",
    "similarity = 'cosine'\n",
    "epochs = 10\n",
    "epoch_start = 0\n",
    "weight_decay = 0.0001\n",
    "# Freeze or not backbone \n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "network = clf.Resnet(num_classes, resnet_arch=resnet_arch, FC_layers=fc_sizes,  \n",
    "            resnet_base=-1, multihead_type='single', base_freeze=freeze)\n",
    "\n",
    "if freeze:\n",
    "    network.base.train(False)\n",
    "    print('freeze backbone')\n",
    "else:\n",
    "    network.base.train(True)\n",
    "    print('dont freeze backbone')\n",
    "\n",
    "network = network.to(device)\n",
    "network_inner = NetworkLatents(network, ['base.8'], pool_factors={'base.8':-1})\n",
    "\n",
    "\n",
    "\n",
    "baseline = (similarity == 'baseline')\n",
    "\n",
    "h = h_dict[similarity](network.classifier_penultimate, num_classes)\n",
    "\n",
    "h = h.to(device)\n",
    "deconf_net = odutils.DeconfNet(network, network.classifier_penultimate, num_classes, h, baseline)\n",
    "deconf_net = deconf_net.to(device)\n",
    "parameters = []\n",
    "h_parameters = []\n",
    "for name, parameter in deconf_net.named_parameters():\n",
    "    if name == 'h.h.weight' or name == 'h.h.bias':\n",
    "        h_parameters.append(parameter)\n",
    "    else:\n",
    "        parameters.append(parameter)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(parameters, lr = 0.1, momentum = 0.9, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [int(epochs * 0.5), int(epochs * 0.75)], gamma = 0.1)\n",
    "\n",
    "h_optimizer = optim.SGD(h_parameters, lr = 0.1, momentum = 0.9) # No weight decay\n",
    "h_scheduler = optim.lr_scheduler.MultiStepLR(h_optimizer, milestones = [int(epochs * 0.5), int(epochs * 0.75)], gamma = 0.1)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dont freeze backbone\n",
      "Will fetch activations from:\n",
      "base.8, average pooled by -1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "network.classifier_penultimate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "deconf_net.train()\n",
    "\n",
    "num_batches = len(train_data)\n",
    "epoch_bar = tqdm(total = num_batches * epochs, initial = num_batches * epoch_start)\n",
    "epoch_loss = None\n",
    "for epoch in range(epoch_start, epochs):\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_inputs = 0\n",
    "    for batch_idx, batch in enumerate(train_data):\n",
    "        if len(batch)>2:\n",
    "            inputs,targets,_=batch\n",
    "        else:\n",
    "            inputs,targets=batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        h_optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits, _, _ = deconf_net(inputs)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        h_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_inputs += inputs.shape[0]\n",
    "        \n",
    "        epoch_bar.set_description(f'Training | Epoch {epoch + 1}/{epochs} | loss = {loss/inputs.shape[0]:0.5f} | Batch {batch_idx + 1}/{num_batches}')\n",
    "        epoch_bar.update()\n",
    "\n",
    "    epoch_loss = total_loss/num_inputs\n",
    "    h_scheduler.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_bar.set_description(f'Training | Epoch {epochs}/{epochs} | Epoch loss = {epoch_loss:0.5f} | Batch {num_batches}/{num_batches}')\n",
    "    epoch_bar.update()\n",
    "\n",
    "epoch_bar.close()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Training | Epoch 10/10 | Epoch loss = 0.00839 | Batch 500/500: : 5010it [04:14, 19.72it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "deconf_net.eval()\n",
    "best_val_score = None\n",
    "best_auc = None\n",
    "\n",
    "\n",
    "def testData(model, CUDA_DEVICE, data_loader, noise_magnitude, criterion, score_func = 'h', title = 'Testing'):\n",
    "    model.eval()\n",
    "    num_batches = len(data_loader)\n",
    "    results = []\n",
    "    data_iter = tqdm(data_loader)\n",
    "    for j, batch in enumerate(data_iter):\n",
    "        images = batch[0]\n",
    "        data_iter.set_description(f'{title} | Processing image batch {j + 1}/{num_batches}')\n",
    "        images = Variable(images.to(CUDA_DEVICE), requires_grad = True)\n",
    "        \n",
    "        \n",
    "        logits, h, g = model(images)\n",
    "\n",
    "        if score_func == 'h':\n",
    "            scores = h\n",
    "        elif score_func == 'g':\n",
    "            scores = g\n",
    "        elif score_func == 'logit':\n",
    "            scores = logits\n",
    "\n",
    "        # Calculating the perturbation we need to add, that is,\n",
    "        # the sign of gradient of the numerator w.r.t. input\n",
    "\n",
    "        max_scores, _ = torch.max(scores, dim = 1)\n",
    "        max_scores.backward(torch.ones(len(max_scores)).to(CUDA_DEVICE))\n",
    "        \n",
    "        # Normalizing the gradient to binary in {-1, 1}\n",
    "        if images.grad is not None:\n",
    "            gradient = torch.ge(images.grad.data, 0)\n",
    "            gradient = (gradient.float() - 0.5) * 2\n",
    "            # Normalizing the gradient to the same space of image\n",
    "            gradient[::, 0] = (gradient[::, 0] )/r_std\n",
    "            gradient[::, 1] = (gradient[::, 1] )/g_std\n",
    "            gradient[::, 2] = (gradient[::, 2] )/b_std\n",
    "            # Adding small perturbations to images\n",
    "            tempInputs = torch.add(images.data, gradient, alpha=noise_magnitude)\n",
    "        \n",
    "            # Now calculate score\n",
    "            logits, h, g = model(tempInputs)\n",
    "\n",
    "            if score_func == 'h':\n",
    "                scores = h\n",
    "            elif score_func == 'g':\n",
    "                scores = g\n",
    "            elif score_func == 'logit':\n",
    "                scores = logits\n",
    "\n",
    "        results.extend(torch.max(scores, dim=1)[0].data.cpu().numpy())\n",
    "        \n",
    "    data_iter.set_description(f'{title} | Processing image batch {num_batches}/{num_batches}')\n",
    "    data_iter.close()\n",
    "\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "\n",
    "noise_magnitudes = [0, 0.0025, 0.005, 0.01, 0.02, 0.04, 0.08]\n",
    "# noise_magnitudes = [0.02000]\n",
    "# score_func='h', ['h', 'g', 'logit']\n",
    "for score_func in ['h', 'g', 'logit']:\n",
    "    print(f'Score function: {score_func}')\n",
    "    for noise_magnitude in noise_magnitudes:\n",
    "        print(f'Noise magnitude {noise_magnitude:.5f}         ')\n",
    "        validation_results =  np.average(testData(deconf_net, device, val_data, noise_magnitude, criterion, score_func, title = 'Validating'))\n",
    "        print('ID Validation Score:',validation_results)\n",
    "        \n",
    "        id_test_results = testData(deconf_net, device, test_data, noise_magnitude, criterion, score_func, title = 'Testing ID') \n",
    "        \n",
    "        ood_test_results = testData(deconf_net, device, open_data, noise_magnitude, criterion, score_func, title = 'Testing OOD')\n",
    "        auroc = calc_auroc(id_test_results, ood_test_results)*100\n",
    "        tnrATtpr = calc_tnr(id_test_results, ood_test_results)\n",
    "        print('AUROC:', auroc, 'TNR@TPR:', tnrATtpr)\n",
    "        \n",
    "        if best_auc is None:\n",
    "            best_auc = auroc\n",
    "        else:\n",
    "            best_auc = max(best_auc, auroc)\n",
    "            \n",
    "        if best_val_score is None or validation_results > best_val_score:\n",
    "            best_val_score = validation_results\n",
    "            best_val_auc = auroc\n",
    "            best_tnr = tnrATtpr\n",
    "\n",
    "\n",
    "print('supposedly best auc: ', best_val_auc, ' and tnr@tpr ', best_tnr)\n",
    "print('true best auc:'      , best_auc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score function: h\n",
      "Noise magnitude 0.00000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.00it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.55577916\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.69it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:05<00:00, 18.03it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6922097  0.6296872  0.32492274 ... 0.46386057 0.46702    0.5168823 ]\n",
      "AUROC: 72.50381611111112 TNR@TPR: 0.0605\n",
      "Noise magnitude 0.00250         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.24it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.6004403\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.48it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.64it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7116208  0.6692249  0.4133578  ... 0.4952236  0.5913173  0.45497993]\n",
      "AUROC: 78.69279388888889 TNR@TPR: 0.10760000000000003\n",
      "Noise magnitude 0.00500         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.91it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.631015\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.31it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.51it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7242788  0.6836441  0.481248   ... 0.45871106 0.59394366 0.50947964]\n",
      "AUROC: 82.58283388888889 TNR@TPR: 0.1482\n",
      "Noise magnitude 0.01000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.14it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.66503614\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.16it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.97it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.73539215 0.68781525 0.57725734 ... 0.47386894 0.48297986 0.6698753 ]\n",
      "AUROC: 84.89083388888888 TNR@TPR: 0.15259999999999996\n",
      "Noise magnitude 0.02000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.48it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.68074\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.24it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.94it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7362242  0.6578405  0.6462285  ... 0.6450399  0.5802274  0.64801353]\n",
      "AUROC: 80.13252888888888 TNR@TPR: 0.04159999999999997\n",
      "Noise magnitude 0.04000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.82it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.66266835\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.27it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.91it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.72351986 0.5447138  0.6187099  ... 0.68599725 0.6693749  0.65842927]\n",
      "AUROC: 60.37273722222223 TNR@TPR: 0.00029999999999996696\n",
      "Noise magnitude 0.08000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.03it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.5952252\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.27it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 16.26it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.6635672  0.32763836 0.38003463 ... 0.63146365 0.6736658  0.6302813 ]\n",
      "AUROC: 42.24797777777778 TNR@TPR: 0.0\n",
      "Score function: g\n",
      "Noise magnitude 0.00000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.46it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.09476013\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.20it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.69it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.08057105 0.08950289 0.1026643  ... 0.09202924 0.09369212 0.08918282]\n",
      "AUROC: 44.94906666666667 TNR@TPR: 0.0\n",
      "Noise magnitude 0.00250         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.18it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.098071106\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.13it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 16.17it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.08145729 0.09493756 0.10411864 ... 0.09900812 0.09836993 0.08736531]\n",
      "AUROC: 50.72075111111112 TNR@TPR: 0.0\n",
      "Noise magnitude 0.00500         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.14it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.100861244\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.95it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.33it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.0823217  0.09975537 0.10506327 ... 0.09454659 0.09289794 0.10414293]\n",
      "AUROC: 55.64913 TNR@TPR: 0.00019999999999997797\n",
      "Noise magnitude 0.01000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.81it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.10495981\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.32it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 16.55it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.08442674 0.10498694 0.10627051 ... 0.09789249 0.09857785 0.1014412 ]\n",
      "AUROC: 61.029562222222225 TNR@TPR: 0.00029999999999996696\n",
      "Noise magnitude 0.02000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 10.79it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.10868364\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.31it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.97it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.08948018 0.10426303 0.10603075 ... 0.1026388  0.10224894 0.1041573 ]\n",
      "AUROC: 58.62560611111112 TNR@TPR: 0.0006000000000000449\n",
      "Noise magnitude 0.04000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.03it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.10887649\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:04<00:00, 18.25it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.86it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.10027935 0.10110487 0.10289487 ... 0.10468382 0.10670757 0.10429157]\n",
      "AUROC: 37.424998333333335 TNR@TPR: 0.0\n",
      "Noise magnitude 0.08000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.52it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 0.10557077\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.90it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 16.40it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.10417359 0.09437548 0.09722435 ... 0.10280968 0.10208116 0.10264105]\n",
      "AUROC: 31.72538777777778 TNR@TPR: 0.0\n",
      "Score function: logit\n",
      "Noise magnitude 0.00000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.38it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 6.4443846\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.94it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.79it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[8.591296  7.0353837 3.1649048 ... 4.108378  4.2370605 6.129511 ]\n",
      "AUROC: 69.13797055555555 TNR@TPR: 0.04179999999999995\n",
      "Noise magnitude 0.00250         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.41it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 7.1110196\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.82it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.76it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[8.802993  7.6889935 4.064237  ... 5.417504  6.373163  6.577016 ]\n",
      "AUROC: 74.1822 TNR@TPR: 0.07869999999999999\n",
      "Noise magnitude 0.00500         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.31it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 7.6270566\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.67it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 16.22it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[8.944961  7.987342  4.7921033 ... 5.8799047 6.2938204 5.18178  ]\n",
      "AUROC: 76.96291944444445 TNR@TPR: 0.09719999999999995\n",
      "Noise magnitude 0.01000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 10.83it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 8.327636\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.95it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.33it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9.075539  8.138698  5.9452734 ... 7.7069736 6.983734  5.060582 ]\n",
      "AUROC: 77.55752833333332 TNR@TPR: 0.08879999999999999\n",
      "Noise magnitude 0.02000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.50it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 8.935108\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.95it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.78it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9.090137 7.666513 7.099595 ... 8.215551 9.180006 8.383198]\n",
      "AUROC: 66.70986666666667 TNR@TPR: 0.01870000000000005\n",
      "Noise magnitude 0.04000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.15it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 9.042468\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.67it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 15.78it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[9.03903   5.9901166 6.856672  ... 9.641879  8.874363  8.985945 ]\n",
      "AUROC: 41.23845055555556 TNR@TPR: 0.0011999999999999789\n",
      "Noise magnitude 0.08000         \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Validating | Processing image batch 10/10: 100%|██████████| 10/10 [00:00<00:00, 11.73it/s]\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID Validation Score: 8.161018\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Testing ID | Processing image batch 90/90: 100%|██████████| 90/90 [00:05<00:00, 17.70it/s]\n",
      "Testing OOD | Processing image batch 100/100: 100%|██████████| 100/100 [00:06<00:00, 14.61it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 8.70383    3.8612165  4.001045  ... 10.263513  10.026801   9.879645 ]\n",
      "AUROC: 27.481771111111115 TNR@TPR: 0.0\n",
      "supposedly best auc:  41.23845055555556  and tnr@tpr  0.0011999999999999789\n",
      "true best auc: 84.89083388888888\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DFM comparisom"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# DFM baseline \n",
    "detector_params = {'pca_level': 0.995, 'score_type': 'pca', 'n_components': None, 'n_percent_comp':0.2, 'device':device, 'target_ind': 1, 'dfm_layers_input': 'base.8'}\n",
    "\n",
    "# fit on first task \n",
    "\n",
    "current_features = futils.extract_features(network_inner, train_data, \\\n",
    "    target_ind=detector_params['target_ind'], homog_ind=1, \n",
    "    device=detector_params['device'])\n",
    "\n",
    "\n",
    "novelty_detector = novel.NoveltyDetector().create_detector(type='dfm', params=detector_params)\n",
    "\n",
    "\n",
    "\n",
    "dfm_x = current_features[0][detector_params['dfm_layers_input']]\n",
    "dfm_y = current_features[1]\n",
    "\n",
    "novelty_detector.fit_total(dfm_x.T, dfm_y)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_comp var 0.995\n",
      "end fit 9.744817018508911\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "epochs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# ------\n",
    "id_test_results, ood_test_results, np.average(id_test_results), np.average(ood_test_results)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([8.70383  , 3.8612165, 4.001045 , ..., 7.180317 , 6.088069 ,\n",
       "        9.403352 ], dtype=float32),\n",
       " array([ 9.999721,  9.991745,  9.857517, ..., 10.263513, 10.026801,\n",
       "         9.879645], dtype=float32),\n",
       " 8.184829,\n",
       " 9.942373)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "r_mean, r_std"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.4913725490196078, 0.24705882352941178)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# all_dsets = dset.call_dataset_holdout('cifar10',  '../../data', './experiments', experiment_filepath=None, \n",
    "#                                             experiment_name='tryout', holdout_percent=0.25,  max_holdout=0.75, scenario='nc', \n",
    "#                                             scenario_classif='class', exp_type='class', num_per_task=1, num_classes_first=2, \n",
    "#                                             shuffle=False, preload=False)\n",
    "\n",
    "# train_datasets, train_holdout_datasets, test_datasets, seq_tasks, dset_prep = all_dsets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# 1) Select subset of old based on mixing degree \n",
    "\n",
    "# Get number_images_old\n",
    "# current_task=2\n",
    "# percent_old = 0.5\n",
    "# num_new = train_datasets[current_task].__len__()\n",
    "# num_old_total = sum([train_holdout_datasets[i].__len__() for i in range(current_task)])\n",
    "\n",
    "# num_old = min(int(percent_old*num_new), num_old_total)\n",
    "# real_percent_old = num_old/num_new\n",
    "\n",
    "# # uniform sampling accross old tasks, can be changed after\n",
    "# num_old_per_task = int(num_old/len(train_holdout_datasets[:current_task]))\n",
    "\n",
    "# print(num_new, num_old, num_old_total, num_old_per_task, real_percent_old)\n",
    "\n",
    "\n",
    "\n",
    "# print(train_holdout_datasets[0].__len__())\n",
    "\n",
    "# # redo the loaders every time \n",
    "# # # 2) Evaluate DFM on old + new of current task and see predictions based on threshold level.\n",
    "# old_gt = []\n",
    "# old_scores = []\n",
    "# num_samples_old = 0\n",
    "# for t_old in range(current_task):\n",
    "#     train_holdout_datasets[t_old].select_random_subset(num_old_per_task)\n",
    "#     num_samples_old += train_holdout_datasets[t_old].__len__()\n",
    "#     t_loader_old = torch.utils.data.DataLoader(train_holdout_datasets[t_old], batch_size=100,\n",
    "#                                             shuffle=True, num_workers=4)\n",
    "#     break\n",
    "#     gt, scores = novelty_detector.score(network_inner, args.input_layer_name, t_loader_old)\n",
    "#     old_gt.append(gt)\n",
    "#     old_scores.append(scores)\n",
    "\n",
    "# # filtered indices accross all old \n",
    "# th = 0.5\n",
    "# inds_above_th = [np.arange(l.shape[0])[l>th] for l in old_scores]\n",
    "# num_fake_new = sum([l.shape[0] for l in inds_above_th])\n",
    "\n",
    "# print('error mistaking old data for new: ', num_fake_new/num_samples_old)\n",
    "\n",
    "\n",
    "# new_gt, new_scores = novelty_detector.score(network_inner, args.input_layer_name, train_loaders[current_task])\n",
    "# inds_below_th_new = np.arange(new_scores.shape[0])[new_scores<=th]\n",
    "# num_fake_old = inds_below_th_new.shape[0]\n",
    "\n",
    "\n",
    "# print('error mistaking new data for old: ', num_fake_old/new_scores.shape[0])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def pl_decision(self, scores, thresh):\n",
    "    scores_max, _ = scores.max(dim=1)\n",
    "    pl_idx_mask = scores_max > (thresh)\n",
    "    pl_idxs = pl_idx_mask.nonzero().view(-1)\n",
    "    no_pl_idx_mask = scores_max <= (thresh)\n",
    "    no_pl_idxs = no_pl_idx_mask.nonzero().view(-1)\n",
    "    return pl_idxs, no_pl_idxs"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('remind_proj': conda)"
  },
  "interpreter": {
   "hash": "05b150e87e16a7f15e68a52cad20b8449e4511c9ed1c6048915793505d6c322d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
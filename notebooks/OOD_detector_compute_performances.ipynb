{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import colors as mcolors\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sklearn.metrics\n",
    "\n",
    "from argparse import Namespace\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "# --------------- Inputs --------------------------------\n",
    "\n",
    "dataset = 'inaturalist21' # dataset name (inaturalist21, cifar10, cifar100, emnist, 8dset)\n",
    "run_name = '120322_163621' # run ID of results folder \n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "results_dir = '../src/novelty_dfm_CL/Results_DFM_CL/%s/'%dataset\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "experiment_path = results_dir +run_name+'/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather performance metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_config_value(j, key):\n",
    "    if key in j:\n",
    "        return j[key]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def compute_f1(precision, recall):\n",
    "    \n",
    "    fscore = (2*(precision*recall))/(precision+recall)\n",
    "    \n",
    "    return fscore\n",
    "\n",
    "\n",
    "\n",
    "def compute_auroc_aupr(results_curves, summary_stats, test=False):\n",
    "    \n",
    "    if test:\n",
    "        summary_stats['auroc_test']={'avg':round(np.mean(results_curves[:, 1]),4), 'last':round(results_curves[-1, 1],4)}\n",
    "        summary_stats['aupr_test']={'avg':round(np.mean(results_curves[:, 2]),4), 'last':round(results_curves[-1, 2],4)}\n",
    "\n",
    "    else:\n",
    "        summary_stats['auroc']={'avg':round(np.mean(results_curves[:, 1]),4), 'last':round(results_curves[-1, 1],4)}\n",
    "        summary_stats['aupr']={'avg':round(np.mean(results_curves[:, 2]),4), 'last':round(results_curves[-1, 2],4)}\n",
    "\n",
    "    return summary_stats\n",
    "\n",
    "\n",
    "def summarize_metric_task_incdfm(results, summary_stats):\n",
    "        \n",
    "    tasks = np.unique(results.T[0,...])\n",
    "    # print('tasks', tasks)\n",
    "    \n",
    "    # Last - metrics \n",
    "    summary_stats['precision']={'last':round(results[-1,-2],4)}\n",
    "    summary_stats['recall']={'last':round(results[-1,-1],4)}\n",
    "    summary_stats['f1']={'last':round(compute_f1(summary_stats['precision']['last'], summary_stats['recall']['last']),4)}\n",
    "\n",
    "    ## Get last value of each task to later compute averages \n",
    "    p_avg_tasks=[]\n",
    "    r_avg_tasks=[]\n",
    "    for t in tasks:\n",
    "        inds_task = np.where(results_iters[:,0]==t)[0]\n",
    "        p_avg_tasks.append(results_iters[inds_task[-1],-2])\n",
    "        r_avg_tasks.append(results_iters[inds_task[-1],-1])\n",
    "        \n",
    "    # print('p_avg_tasks', p_avg_tasks)\n",
    "    # print('r_avg_tasks', r_avg_tasks)\n",
    "       \n",
    "    \n",
    "    summary_stats['precision']['avg']=round(np.mean(p_avg_tasks),4)\n",
    "    summary_stats['recall']['avg']=round(np.mean(r_avg_tasks),4)\n",
    "    summary_stats['f1']['avg']=round(compute_f1(summary_stats['precision']['avg'], summary_stats['recall']['avg']),4)\n",
    "\n",
    "    return summary_stats\n",
    "    \n",
    "    \n",
    "    \n",
    "def parse_results(results_iters, ood_name, summary_stats):\n",
    "\n",
    "    ## P, R and f1\n",
    "    if ood_name=='incdfm':\n",
    "        summary_stats = summarize_metric_task_incdfm(results_iters, summary_stats)\n",
    "    else:\n",
    "        summary_stats['precision']={'avg':round(np.mean(results_iters[:, -2]),4), 'last':round(results_iters[-1, -2],4)}\n",
    "        summary_stats['recall']={'avg':round(np.mean(results_iters[:, -1]),4), 'last':round(results_iters[-1, -1],4)}\n",
    "        summary_stats['f1']={'avg':round(compute_f1(summary_stats['precision']['avg'], summary_stats['recall']['avg']),4), \\\n",
    "            'last':round(compute_f1(summary_stats['precision']['last'], summary_stats['recall']['last']),4)}\n",
    "\n",
    "    ## Auroc, Aupr, Auroc_test, Aupr_test\n",
    "    \n",
    "\n",
    "    return summary_stats\n",
    "\n",
    "\n",
    "def compute_acc_avg(accs, summary_stats):\n",
    "    tasks = np.unique(accs.T[0,...])\n",
    "    epochs = np.unique(accs.T[1,...])\n",
    "    \n",
    "    num_epochs = epochs.shape[0]\n",
    "    \n",
    "    accs_last = accs.T[-1,...][-num_epochs:]\n",
    "    \n",
    "    summary_stats['acc_last_clf']=np.max(accs_last)\n",
    "    \n",
    "    return summary_stats\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No acc_avg file /lab/arios/ProjIntel/incDFM/src/novelty_dfm_CL/Results_DFM_CL/inaturalist21/120322_163621/\n",
      "{'precision': {'avg': 0.6031, 'last': 0.6113}, 'recall': {'avg': 0.7366, 'last': 0.6564}, 'f1': {'avg': 0.6632, 'last': 0.633}, 'auroc_test': {'avg': 0.6038, 'last': 0.5545}, 'aupr_test': {'avg': 0.6131, 'last': 0.565}}\n"
     ]
    }
   ],
   "source": [
    "summary_stats={}\n",
    "    \n",
    "    \n",
    "with open(Path(experiment_path) / Path(\"config_model.json\"), \"r\") as config_file:\n",
    "    j = json.load(config_file)\n",
    "# correct version mismatch for naming\n",
    "ood_name = get_config_value(j, \"novelty_detector_name\")\n",
    "ood_th = get_config_value(j, \"threshold_type\")\n",
    "if (ood_name=='dfm') and (ood_th=='iter'):\n",
    "    ood_name = 'incdfm'\n",
    "\n",
    "\n",
    "try:\n",
    "    results_iters = np.genfromtxt(Path(experiment_path) / Path(\"novelty_accuracies_iter.txt\"), delimiter=' ', dtype=float, skip_header=1)\n",
    "    if results_iters.ndim == 1:\n",
    "        print('No novelty_accuracies_iter file', experiment_path)\n",
    "    summary_stats = parse_results(results_iters, ood_name, summary_stats)\n",
    "except:\n",
    "    print('No novelty_accuracies_iter file', experiment_path)\n",
    "        \n",
    "        \n",
    "\n",
    "# 2) ------- get Auroc/Aupr for holdout_old x train_new\n",
    "try:\n",
    "    results_curves = np.genfromtxt(Path(experiment_path) / Path(\"novelty_eval.txt\"), delimiter=' ', dtype=float, skip_header=1)\n",
    "    if results_curves.ndim == 1:\n",
    "        print('No novelty_eval file', experiment_path)\n",
    "        summary_stats = compute_auroc_aupr(results_curves, summary_stats, test=False)\n",
    "except:\n",
    "    ## Will probably be multitask runs \n",
    "    ## separate those from traditional CL runs in a separate folder \n",
    "    print('No novelty_eval file', experiment_path)\n",
    "\n",
    "    \n",
    "\n",
    "# 2) ------- get Auroc_test/Aupr_test for test_old x test_new\n",
    "try:\n",
    "    results_curves = np.genfromtxt(Path(experiment_path) / Path(\"novelty_eval_test.txt\"), delimiter=' ', dtype=float, skip_header=1)\n",
    "    if results_curves.ndim == 1:\n",
    "        print('No novelty_eval_test file', experiment_path)\n",
    "    summary_stats = compute_auroc_aupr(results_curves, summary_stats, test=True)    \n",
    "except:\n",
    "    print('No novelty_eval_test file', experiment_path)\n",
    "    \n",
    "    \n",
    "try:\n",
    "    results_acc_clf = np.genfromtxt(Path(experiment_path) / Path(\"acc_avg.txt\"), delimiter=' ', dtype=float)\n",
    "    summary_stats = compute_acc_avg(results_acc_clf, summary_stats)    \n",
    "except:\n",
    "    print('No acc_avg file', experiment_path)\n",
    "\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

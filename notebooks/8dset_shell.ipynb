{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as TF\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "from torchvision import datasets, models, transforms\n",
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import tforms\n",
    "import utils\n",
    "sys.path.append('../src/novelty_dfm_CL/')\n",
    "\n",
    "from dset8_specific_scripts.eightdset_wrappers.aircraft import MY_AIRCRAFT\n",
    "from dset8_specific_scripts.eightdset_wrappers.birds import MY_BIRDS\n",
    "from dset8_specific_scripts.eightdset_wrappers.cars import MY_CARS\n",
    "from dset8_specific_scripts.eightdset_wrappers.voc import MY_VOC\n",
    "from dset8_specific_scripts.eightdset_wrappers.char import MY_CHAR\n",
    "from dset8_specific_scripts.eightdset_wrappers.flowers import MY_FLOWERS\n",
    "from dset8_specific_scripts.eightdset_wrappers.scenes import MY_SCENES\n",
    "from dset8_specific_scripts.eightdset_wrappers.svhn import MY_SVHN\n",
    "\n",
    "\n",
    "## Transform to resize images \n",
    "class Resize_tf():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(299), TF.CenterCrop(299),\n",
    "                            #   inaturalist_normalize(),\n",
    "                              ])\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "transform = Resize_tf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for SHELL\n",
    "\n",
    "### 1) half of classes of each dataset\n",
    "### 2) resize to 299 and 124or128??? Ask Laurent \n",
    "### 3) Save them to a tmpig21 folder so that all can access \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_classes(all_labels_dset, percent_classes_keep=0.5):\n",
    "    \n",
    "    classes = np.unique(all_labels_dset)\n",
    "    \n",
    "    num_classes = classes.shape[0]\n",
    "    \n",
    "    num_keep = int(np.round(percent_classes_keep*num_classes))\n",
    "    \n",
    "    classes_keep = np.random.permutation(num_classes)[:num_keep]\n",
    "    \n",
    "    indices_keep = []\n",
    "    for c in classes_keep:\n",
    "        indices_keep.extend(np.where(all_labels_dset==c)[0].tolist())\n",
    "        \n",
    "    return np.array(indices_keep)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Shell_8dset_prepare_test(dataroot, outdir, experiment_name, percent_classes_keep=0.5):\n",
    "    \"\"\" \n",
    "    Experiment that keeps half of 8dset \n",
    "    \"\"\"\n",
    "    \n",
    "    list_tasks = ['flowers', 'scenes', 'birds', 'cars', 'aircraft', 'voc', 'chars', 'svhn']\n",
    "    individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "    \n",
    "    tasklists_train = ['%s/%s/labels/val.txt'%(dataroot,d) for d in list_tasks[:-1]]+['test']\n",
    "\n",
    "    dsets = []\n",
    "    for i, dname in enumerate(list_tasks):\n",
    "        if dname =='svhn':\n",
    "            dataset = MY_SVHN(img_path='/lab/arios/ProjIntel/incDFM/data/svhn/',\n",
    "                                    split='val')\n",
    "        else:\n",
    "            dataset = individual_wrappers[dname](img_path='%s/%s'%(dataroot, dname),\n",
    "                                            txt_path=tasklists_train[i],\n",
    "                                            dataset='val')\n",
    "        dsets.append(dataset)\n",
    "        \n",
    "        \n",
    "    # filter classes     \n",
    "    # --- Set up Task sequences 8dset\n",
    "    tasks_list_test=[]\n",
    "    for t, task in enumerate(list_tasks):\n",
    "        \n",
    "        labels = np.array(dsets[t].img_label)\n",
    "        \n",
    "        print('original number', labels.shape[0])\n",
    "        \n",
    "        indices_task = filter_classes(labels, percent_classes_keep=percent_classes_keep)\n",
    "        \n",
    "        tasks_list_test.append(indices_task)\n",
    "        \n",
    "        print(tasks_list_test[-1].shape)\n",
    "\n",
    "    # --- save sequences to text files \n",
    "    task_filepaths_test = saveTasks_to_txt(tasks_list_test, tasklists_train, '8dset', 'test', outdir, experiment_name, scenario='nc')\n",
    "\n",
    "    return task_filepaths_test\n",
    "        \n",
    "\n",
    "\n",
    "def Shell_8dset_prepare_train_val_split(dataroot, outdir, experiment_name,\\\n",
    "    percent_classes_keep=0.5, validation_percent=0.2):\n",
    "    \"\"\" \n",
    "    Only do holdout for Train\n",
    "    Args:\n",
    "        holdout_percent (float): percent of train data to leave out for later\n",
    "        max_holdout (float): maximum holdout_percent allowed. Usually not changed \n",
    "        root (string): Root directory of the dataset where images and paths file are stored\n",
    "        outdir (string): Out directory to store experiment task files (txt sequences of objects)\n",
    "        train (bool, optional): partition set. If train=True then it is train set. Else, test. \n",
    "        scenario (string, optional): What tye of CL learning regime. 'nc' stands for class incremental,\\\n",
    "             where each task contains disjoint class subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    list_tasks = ['flowers', 'scenes', 'birds', 'cars', 'aircraft', 'voc', 'chars', 'svhn']\n",
    "    individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "    \n",
    "    tasklists_train = ['%s/%s/labels/train.txt'%(dataroot,d) for d in list_tasks[:-1]]+['train']\n",
    "\n",
    "    dsets = []\n",
    "    for i, dname in enumerate(list_tasks):\n",
    "        if dname =='svhn':\n",
    "            dataset = MY_SVHN(img_path='/lab/arios/ProjIntel/incDFM/data/svhn/',\n",
    "                                    split='train')\n",
    "        else:\n",
    "            dataset = individual_wrappers[dname](img_path='%s/%s'%(dataroot, dname),\n",
    "                                            txt_path=tasklists_train[i],\n",
    "                                            dataset='train')\n",
    "        dsets.append(dataset)\n",
    "        \n",
    "                \n",
    "    # --- Set up Task sequences 8dset\n",
    "    tasks_list_train=[]\n",
    "    tasks_list_val=[]\n",
    "    for t, task in enumerate(list_tasks):\n",
    "        \n",
    "        labels = np.array(dsets[t].img_label)\n",
    "        \n",
    "        print('original number', labels.shape[0])\n",
    "        \n",
    "        indices_task = filter_classes(labels, percent_classes_keep=percent_classes_keep)\n",
    "        \n",
    "        num_samples_task = indices_task.shape[0]\n",
    "        \n",
    "        inds_shuf = np.random.permutation(indices_task)\n",
    "        \n",
    "        # divide the train/val/holdout split\n",
    "        split_val = int(np.floor(validation_percent*num_samples_task))\n",
    "        tasks_list_val.append(inds_shuf[:split_val])\n",
    "        inds_train = inds_shuf[split_val:]\n",
    "        tasks_list_train.append(inds_train)\n",
    "\n",
    "        print(tasks_list_train[-1].shape, tasks_list_val[-1].shape)\n",
    "\n",
    "\n",
    "    # --- save sequences to text files \n",
    "    task_filepaths_train = saveTasks_to_txt(tasks_list_train, tasklists_train, '8dset', 'train', outdir, experiment_name, scenario='nc')\n",
    "    task_filepaths_val = saveTasks_to_txt(tasks_list_val, tasklists_train, '8dset', 'validation', outdir, experiment_name, scenario='nc')\n",
    "\n",
    "\n",
    "    return task_filepaths_train, task_filepaths_val\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def saveTasks_to_txt(tasklists_subset_indices, tasklists_orig, dset_name, partition, outdir, experiment_name, scenario='nc'):\n",
    "    \n",
    "    # --- save sequences to text files \n",
    "    dest_dir = '%s/%s/%s'%(outdir, dset_name, experiment_name)\n",
    "    utils.makedirectory(dest_dir)\n",
    "    dest_dir = dest_dir + '/%s'%(partition)\n",
    "    utils.makedirectory(dest_dir)\n",
    "\n",
    "    # Create directory for experiment \n",
    "    task_filepaths=[]\n",
    "    for task in range(len(tasklists_orig)):\n",
    "        subset_indices = tasklists_subset_indices[task]\n",
    "        if task<7:\n",
    "            task_filepaths.append(\"%s/%s_%s_task_%d.txt\"%(dest_dir, scenario, partition, task))\n",
    "            write_to_file_subset_8dset(tasklists_orig[task], subset_indices, task_filepaths[-1])\n",
    "        else:\n",
    "            task_filepaths.append(\"%s/%s_%s_task_%d.npy\"%(dest_dir, scenario, partition, task))\n",
    "            np.save(task_filepaths[-1], subset_indices)\n",
    "        \n",
    "    return task_filepaths\n",
    "\n",
    "\n",
    "\n",
    "def write_to_file_subset_8dset(original_txt, subset_indices, new_txt):\n",
    "    with open(original_txt, \"r\") as file_input:\n",
    "        with open(new_txt, \"w\") as output: \n",
    "            for i, line in enumerate(file_input):\n",
    "                if i in subset_indices:\n",
    "                    output.write(line)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number 6149\n",
      "(2506,) (626,)\n",
      "original number 5359\n",
      "(2169,) (542,)\n",
      "original number 5994\n",
      "(2399,) (599,)\n",
      "original number 8144\n",
      "(3292,) (823,)\n",
      "original number 3334\n",
      "(1256,) (314,)\n",
      "original number 1683\n",
      "(773,) (193,)\n",
      "original number 4623\n",
      "(1703,) (425,)\n",
      "original number 73257\n",
      "(23619,) (5904,)\n"
     ]
    }
   ],
   "source": [
    "dataroot = '../data/8dset/'\n",
    "outdir = '/lab/tmpig21/u/arios/8dset_shell/shell_experiments/'\n",
    "experiment_name='Shell_HalfClasses_8dset'\n",
    "\n",
    "\n",
    "task_filepaths_train, tasks_filepaths_val = Shell_8dset_prepare_train_val_split(dataroot, outdir, experiment_name,\\\n",
    "    percent_classes_keep=0.5, validation_percent=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original number 2040\n",
      "(1020,)\n",
      "original number 1340\n",
      "(687,)\n",
      "original number 5794\n",
      "(2872,)\n",
      "original number 8041\n",
      "(4004,)\n",
      "original number 3333\n",
      "(1404,)\n",
      "original number 1651\n",
      "(872,)\n",
      "original number 1541\n",
      "(839,)\n",
      "original number 26032\n",
      "(10265,)\n"
     ]
    }
   ],
   "source": [
    "dataroot = '../data/8dset/'\n",
    "outdir = '/lab/tmpig21/u/arios/8dset_shell/shell_experiments/'\n",
    "experiment_name='Shell_HalfClasses_8dset'\n",
    "\n",
    "\n",
    "task_filepaths = Shell_8dset_prepare_test(dataroot, outdir, experiment_name, percent_classes_keep=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eightdset_Experiments_w_holdout_w_validation_trainset(dataroot, outdir, experiment_name, dset_name='8dset',\\\n",
    "    holdout_percent=0.2, validation_percent=0.1):\n",
    "    \"\"\" \n",
    "    Only do holdout for Train\n",
    "    Args:\n",
    "        holdout_percent (float): percent of train data to leave out for later\n",
    "        max_holdout (float): maximum holdout_percent allowed. Usually not changed \n",
    "        root (string): Root directory of the dataset where images and paths file are stored\n",
    "        outdir (string): Out directory to store experiment task files (txt sequences of objects)\n",
    "        train (bool, optional): partition set. If train=True then it is train set. Else, test. \n",
    "        scenario (string, optional): What tye of CL learning regime. 'nc' stands for class incremental,\\\n",
    "             where each task contains disjoint class subsets\n",
    "    \"\"\"\n",
    "    \n",
    "    list_tasks = ['flowers', 'scenes', 'birds', 'cars', 'aircraft', 'voc', 'chars', 'svhn']\n",
    "    individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "    \n",
    "    tasklists_train = ['%s/%s/labels/train.txt'%(dataroot,d) for d in list_tasks[:-1]]+['train']\n",
    "\n",
    "    dsets = []\n",
    "    for i, dname in enumerate(list_tasks):\n",
    "        if dname =='svhn':\n",
    "            dataset = MY_SVHN(img_path='/lab/arios/ProjIntel/incDFM/data/svhn/',\n",
    "                                    split='train')\n",
    "        else:\n",
    "            dataset = individual_wrappers[dname](img_path='%s/%s'%(dataroot, dname),\n",
    "                                            txt_path=tasklists_train[i],\n",
    "                                            dataset='train')\n",
    "        dsets.append(dataset)\n",
    "        \n",
    "                \n",
    "    # --- Set up Task sequences 8dset\n",
    "    tasks_list_train=[]\n",
    "    tasks_list_val=[]\n",
    "    tasks_list_holdout=[]\n",
    "    for t, task in enumerate(list_tasks):\n",
    "        \n",
    "        labels = np.array(dsets[t].img_label)\n",
    "        print('Dset %s - Num data %d Num labels %d'%(task, labels.shape[0], np.unique(labels).shape[0]))\n",
    "        num_samples_task = labels.shape[0]\n",
    "        indices_task = np.arange(num_samples_task)\n",
    "\n",
    "        # for each label subset\n",
    "        print('num_samples_task', num_samples_task)\n",
    "        \n",
    "        inds_shuf = np.random.permutation(indices_task)\n",
    "        \n",
    "        # divide the train/val/holdout split\n",
    "        split_val = int(np.floor(validation_percent*num_samples_task))\n",
    "        tasks_list_val.append(inds_shuf[:split_val])\n",
    "        inds_train = inds_shuf[split_val:]\n",
    "\n",
    "\n",
    "        split_holdout = int(np.floor(holdout_percent*num_samples_task))\n",
    "        tasks_list_train.append(inds_train[split_holdout:])\n",
    "        tasks_list_holdout.append(inds_train[:split_holdout])\n",
    "\n",
    "        print(tasks_list_train[-1].shape, tasks_list_val[-1].shape, tasks_list_holdout[-1].shape)\n",
    "\n",
    "    # sys.exit()\n",
    "\n",
    "    # --- save sequences to text files \n",
    "    task_filepaths_train = saveTasks_to_txt(tasks_list_train, tasklists_train, dset_name, 'train', outdir, experiment_name, scenario='nc')\n",
    "    task_filepaths_train_holdout = saveTasks_to_txt(tasks_list_holdout, tasklists_train, dset_name, 'holdout', outdir, experiment_name, scenario='nc')\n",
    "    task_filepaths_val = saveTasks_to_txt(tasks_list_val, tasklists_train, dset_name, 'validation', outdir, experiment_name, scenario='nc')\n",
    "\n",
    "\n",
    "    return task_filepaths_train, task_filepaths_train_holdout, task_filepaths_val\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Shell8dset_Wrapper():\n",
    "    def __init__(self, dataroot, dset_name, split='train', tasklist='task_indices.txt', transform=None, \\\n",
    "        returnIDX=False):\n",
    "        '''\n",
    "        dataroot - for 8dset /lab/arios/ProjIntel/incDFM/data/8dset\n",
    "        dset_name - among the 8 datasets (flowers, aircrafts, birds, cars, char, scenes, voc, svhn)\n",
    "        '''\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.dset_name = dset_name\n",
    "        \n",
    "        self.individual_wrappers={'flowers':MY_FLOWERS, 'aircraft':MY_AIRCRAFT, 'birds': MY_BIRDS, \\\n",
    "            'voc':MY_VOC, 'cars': MY_CARS, 'svhn': MY_SVHN, 'chars':MY_CHAR, 'scenes':MY_SCENES}\n",
    "        \n",
    "        self.order_tasks = {'flowers':0, 'scenes':1, 'birds':2, 'cars':3, 'aircraft':4, 'voc':5,  'chars':6, 'svhn':7}\n",
    "        \n",
    "        self.task_lb = self.order_tasks[self.dset_name]\n",
    "        \n",
    "        if dset_name !='svhn':\n",
    "            self.intern_dset = self.individual_wrappers[dset_name](img_path='%s/%s'%(dataroot, dset_name),\n",
    "                                        txt_path=tasklist,\n",
    "                                        data_transforms=transform)\n",
    "        else:\n",
    "            self.intern_dset = self.individual_wrappers[dset_name]('%s/%s'%(dataroot, dset_name),\n",
    "                                        tasklist=tasklist,\n",
    "                                        data_transforms=transform,\n",
    "                                        split=split)\n",
    "            \n",
    "                        \n",
    "            \n",
    "        self.returnIDX = returnIDX\n",
    "        \n",
    "        self.indices_task_init = np.arange(self.intern_dset.__len__())\n",
    "        \n",
    "        self.indices_task = copy.deepcopy(self.indices_task_init)\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.indices_task.shape[0]\n",
    "\n",
    "    def select_random_subset(self, random_num):\n",
    "\n",
    "        inds_keep = np.random.permutation(np.arange(self.indices_task_init.shape[0]))[:random_num]\n",
    "\n",
    "        self.indices_task = self.indices_task_init[inds_keep]\n",
    "        \n",
    "    def select_specific_subset(self, indices_select):\n",
    "        \n",
    "        self.indices_task = self.indices_task_init[indices_select]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        idx = self.indices_task[idx]\n",
    "\n",
    "        im, class_lbl = self.intern_dset.__getitem__(idx)\n",
    "        \n",
    "        if self.returnIDX:\n",
    "            return im, class_lbl, self.task_lb, self.task_lb, idx\n",
    "            \n",
    "        return im, class_lbl, self.task_lb, self.task_lb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "## for svhn save list with right indices \n",
    "\n",
    "class MY_SVHN(Dataset):\n",
    "    def __init__(self, img_path, indicespath=None, data_transforms=None, target_transform=None, split='train', balance=False):\n",
    "        '''\n",
    "        \n",
    "        indicespath will contain .npy array with indices for half of the classes in svhn. \n",
    "        So total classes will be 5 \n",
    "        '''\n",
    "        \n",
    "        self.train_map = {'train':True, 'val':False}\n",
    "        \n",
    "        self.balance=balance\n",
    "        \n",
    "        self.max_labels=10\n",
    "\n",
    "        self.images, self.img_label = load_SVHN(img_path, self.train_map[split], balance=self.balance)\n",
    "        \n",
    "        # subset \n",
    "        if indicespath is not None:\n",
    "            tasklist = np.load(indicespath)\n",
    "            self.images = self.images[tasklist,...]\n",
    "            self.img_label = self.img_label[tasklist]\n",
    "            \n",
    "        self.img_name = np.arange(self.img_label.shape[0])\n",
    "                \n",
    "        self.data_transforms = data_transforms\n",
    "        self.target_transform=target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_name.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        \n",
    "        idx = self.img_name[item]\n",
    "        \n",
    "        img = self.images[idx,:]\n",
    "        label = self.img_label[idx]\n",
    "        \n",
    "\n",
    "        if self.data_transforms is not None:\n",
    "            try:\n",
    "\n",
    "                img = self.data_transforms(img)\n",
    "                \n",
    "            except:\n",
    "\n",
    "                print(\"Cannot transform image\")\n",
    "                \n",
    "        if self.target_transform is not None:\n",
    "            \n",
    "            label = self.target_transform(label)\n",
    "                \n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "def load_SVHN(root, train=True, balance=True):\n",
    "\n",
    "    root = os.path.expanduser(root)\n",
    "    \n",
    "    if train==True:\n",
    "        filename = \"train_32x32.mat\"\n",
    "    else:\n",
    "        filename = \"test_32x32.mat\"\n",
    "\n",
    "        \n",
    "    import scipy.io as sio\n",
    "    # reading(loading) mat file as array\n",
    "    loaded_mat = sio.loadmat(os.path.join(root, filename))\n",
    "\n",
    "    data = loaded_mat['X']\n",
    "    # loading from the .mat file gives an np array of type np.uint8\n",
    "    # converting to np.int64, so that we have a LongTensor after\n",
    "    # the conversion from the numpy array\n",
    "    # the squeeze is needed to obtain a 1D tensor\n",
    "    labels = loaded_mat['y'].astype(np.int64).squeeze()\n",
    "\n",
    "    # the svhn dataset assigns the class label \"10\" to the digit 0\n",
    "    # this makes it inconsistent with several loss functions\n",
    "    # which expect the class labels to be in the range [0, C-1]\n",
    "    np.place(labels, labels == 10, 0)\n",
    "    data = np.transpose(data, (3, 2, 0, 1))\n",
    "\n",
    "    if train==True:\n",
    "        max_ind_b = 4948\n",
    "    else:\n",
    "        max_ind_b = 1595\n",
    "    \n",
    "    \n",
    "    if balance==True:\n",
    "        \n",
    "        inds_b = []\n",
    "        random.seed(999)\n",
    "\n",
    "        for i in range(10):\n",
    "\n",
    "            arr = np.where(labels==i)[0]\n",
    "            np.random.shuffle(arr)\n",
    "            inds = arr[:max_ind_b]\n",
    "            inds_b.extend(list(inds))\n",
    "\n",
    "        inds_b = np.array(inds_b)\n",
    "        inds_b = inds_b.astype(int)\n",
    "\n",
    "        np.random.shuffle(inds_b)\n",
    "\n",
    "        data = data[inds_b,...]\n",
    "        labels = labels[inds_b]\n",
    "        \n",
    "        \n",
    "    data = torch.from_numpy(data).type(torch.FloatTensor)\n",
    "    # y_vec = torch.from_numpy(y_vec).type(torch.FloatTensor)\n",
    "    labels = torch.from_numpy(labels).type(torch.LongTensor)\n",
    "        \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6656ba487fb895a60e5d4c5030343c34c763d6fe30d1a6683b717bdb1af8ff9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('incDFM_proj')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import colors as mcolors\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "import itertools\n",
    "\n",
    "sys.path.append('../src')\n",
    "import tforms\n",
    "import feature_extraction.feature_extraction_utils as futils\n",
    "from feature_extraction.Network_Latents_Wrapper import NetworkLatents\n",
    "import classifier as clf\n",
    "\n",
    "import novelty_ODD.novelty_detector as novel\n",
    "import novelty_ODD.novelty_eval as novelval \n",
    "\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose between 1 ID and 1 OD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataset(task_subset, dset_name, dir_data, dir_save, subsample_num, train=True):\n",
    "\n",
    "    if dset_name=='cifar10':\n",
    "        import cifar10_dataset as dset  \n",
    "        num_labels = 10\n",
    "        tform_apply = tforms.cifar_train()\n",
    "        # tform_apply = tforms.tf_simple()\n",
    "        task_filepaths = dset.cifar10Experiments([task_subset], dir_data +'cifar10/', dir_save, 'debug', train=train, scenario='nc', shuffle=False)\n",
    "        dataset = dset.cifar10Task(dir_data +'cifar10/', tasklist=task_filepaths[0], transform=tform_apply, train=train)\n",
    "        run_extractor=True\n",
    "        if subsample_num>0:\n",
    "            dataset.select_random_subset(subsample_num)\n",
    "        target_ind = -1\n",
    "        homog_ind = -2\n",
    "\n",
    "    elif dset_name=='svhn':\n",
    "        import svhn_dataset as dset  \n",
    "        num_labels = 10\n",
    "        tform_apply = tforms.svhn_train()\n",
    "        # tform_apply = tforms.tf_simple()\n",
    "        task_filepaths = dset.svhnExperiments([task_subset], dir_data +'svhn/', dir_save, 'debug', train=train, scenario='nc', shuffle=False)\n",
    "        dataset = dset.svhnTask(dir_data +'svhn/', tasklist=task_filepaths[0], transform=tform_apply, train=train)\n",
    "        run_extractor=True\n",
    "        if subsample_num>0:\n",
    "            dataset.select_random_subset(subsample_num)\n",
    "        target_ind = -1\n",
    "        homog_ind = -2\n",
    "\n",
    "    return dataset, target_ind, homog_ind, num_labels\n",
    "\n",
    "\n",
    "\n",
    "def get_features(dataset, network_inner, num_labels, target_ind, extractor_name, device=0):\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=50,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "    start = time.time()\n",
    "    print('feat extraction begin')\n",
    "    current_features = futils.extract_features(network_inner, loader, \\\n",
    "            target_ind=target_ind, homog_ind=-2, device=device, use_raw_images=False, raw_image_transform=None)\n",
    "\n",
    "    print('feat extraction done', time.time()-start)\n",
    "    feat_name = 'base.8'\n",
    "\n",
    "    X = current_features[0][feat_name]\n",
    "    Y = current_features[-2]\n",
    "\n",
    "    return X, Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get ID and OD features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load contrastive backbone\n",
      "Will fetch activations from:\n",
      "base.8, average pooled by -1\n",
      "Get ID/OD datasets\n",
      "get features ID/OD\n",
      "feat extraction begin\n",
      "feat extraction done 38.71662473678589\n",
      "feat extraction begin\n",
      "feat extraction done 36.22305989265442\n"
     ]
    }
   ],
   "source": [
    "ID_dset = 'cifar10'\n",
    "OD_dset = 'svhn'\n",
    "# ID_subset = [0,1]\n",
    "ID_subset = [i for i in range(10)]\n",
    "OD_subset = [i for i in range(10)]\n",
    "# OD_subset=[3]\n",
    "# OD_subset = [9]\n",
    "subsample_num_ID = -1\n",
    "subsample_num_OD=-1\n",
    "extractor_name = 'resnet50_contrastive'\n",
    "dir_data = '/home/amandari/CodeDev/ProjIntel/data/'\n",
    "dir_save = '/home/amandari/CodeDev/ProjIntel/sandbox/random_files/'\n",
    "device = 0\n",
    "\n",
    "surrogate_num_labels=1\n",
    "\n",
    "network = clf.Resnet(surrogate_num_labels, resnet_arch=extractor_name, FC_layers=[],  \n",
    "            resnet_base=-1, multihead_type='single', base_freeze=True, pretrained_weights=None)\n",
    "network = network.to(device)\n",
    "# run extractor \n",
    "network_inner = NetworkLatents(network, ['base.8'], pool_factors={'base.8':-1})\n",
    "\n",
    "\n",
    "print('Get ID/OD datasets')\n",
    "dset_ID, target_ind_ID, _, num_labels_ID = get_dataset(ID_subset, ID_dset, dir_data, dir_save, subsample_num_ID, train=True)\n",
    "dset_ID_test, _, _, _ = get_dataset(ID_subset, ID_dset, dir_data, dir_save, subsample_num_ID, train=False)\n",
    "dset_OD, target_ind_OD, _, num_labels_OD = get_dataset(OD_subset, OD_dset, dir_data, dir_save, subsample_num_OD, train=True)\n",
    "\n",
    "\n",
    "print('get features ID/OD')\n",
    "Feats_ID = get_features(dset_ID, network_inner, num_labels_ID, target_ind_ID, extractor_name, device=device)\n",
    "\n",
    "Feats_OD = get_features(dset_OD, network_inner, num_labels_OD, target_ind_OD, extractor_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit ID data on novelty detector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_comp var 0.995\n",
      "end fit 10.899869680404663\n"
     ]
    }
   ],
   "source": [
    "detector_params = {'pca_level':0.995, 'score_type':'pca', 'n_components': None, 'n_percent_comp': 0.2}\n",
    "\n",
    "detector_params['target_ind']=target_ind_ID\n",
    "detector_params['device']=device\n",
    "\n",
    "novelty_detector = novel.NoveltyDetector().create_detector(type='dfm', params=detector_params)\n",
    " \n",
    "novelty_detector.fit_total(Feats_ID[0].T, Feats_ID[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get score on OD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: PCA(n_components=0.995),\n",
       " 1: PCA(n_components=0.995),\n",
       " 2: PCA(n_components=0.995),\n",
       " 3: PCA(n_components=0.995),\n",
       " 4: PCA(n_components=0.995),\n",
       " 5: PCA(n_components=0.995),\n",
       " 6: PCA(n_components=0.995),\n",
       " 7: PCA(n_components=0.995),\n",
       " 8: PCA(n_components=0.995),\n",
       " 9: PCA(n_components=0.995)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novelty_detector.pca_mats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get scores for novelty detector\n",
      "accuracy new 0.11327275314590067\n",
      "accuracy old 0.6386\n",
      "auroc 0.9763680256550995\n",
      "New task 1, AUROC = 0.976, AUPR = 0.995, AUPR_NORM = 0.995\n",
      "new_scores [51.12514114 45.98828888 20.08731461 ... 31.18333817 10.36089897\n",
      " 12.85968399]\n",
      "old_scores [4.18988514 3.1307559  7.6462245  ... 4.37493992 1.59661889 2.68742704]\n",
      "DFM Results -  Auroc 0.976, Aupr 0.995, Aupr_norm 0.995\n",
      "Average Accuracy per class old 0.6386\n"
     ]
    }
   ],
   "source": [
    "# Wrap data in loader format \n",
    "OD_loader = torch.utils.data.DataLoader(dset_OD, batch_size=100, shuffle=True, num_workers=4)\n",
    "ID_loaders_test = [torch.utils.data.DataLoader(dset_ID_test, batch_size=100, shuffle=True, num_workers=4)]\n",
    "\n",
    "print('Get scores for novelty detector')\n",
    "noveltyResults = novelval.save_novelty_results(1, 'debug', dir_save)\n",
    "# train_loaders_new_only evaluates only current new data (unseen by classifier/main model). \n",
    "params_score = {'layer':'base.8', 'feature_extractor':network_inner, 'base_apply_score':True, 'target_ind':target_ind_OD}\n",
    "# print('new', next(iter(train_loaders_new_only[t])))\n",
    "novelval.evaluate_dfm(1, novelty_detector, noveltyResults, params_score, OD_loader, ID_loaders_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9763680256550995]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noveltyResults.auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05b150e87e16a7f15e68a52cad20b8449e4511c9ed1c6048915793505d6c322d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('remind_proj': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

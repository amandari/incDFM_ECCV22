{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as TF\n",
    "from collections import Counter\n",
    "from tqdm import tqdm \n",
    "# print(torch.cuda.is_available(), torch.cuda.current_device())\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "from inaturalist_dataset import WrapNaturalist\n",
    "    \n",
    "class inaturalist_normalize():\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "\n",
    "    \n",
    "class inaturalist_train():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.RandomHorizontalFlip(), TF.ToTensor(), \n",
    "                              inaturalist_normalize(),\n",
    "                              ])\n",
    "        # self.tf = TF.Compose([core50_normalize()])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "    \n",
    "    \n",
    "def exclude_problem_indices_inaturalist_2019(dataroot,transform=None):\n",
    "    \n",
    "    dataset = datasets.INaturalist(dataroot, version='2019', target_type=['super'], \\\n",
    "                                transform=transform, target_transform=None, download=False)\n",
    "    \n",
    "    indices_all = np.arange(dataset.__len__())\n",
    "    exclude_inds = []\n",
    "    \n",
    "    for i, ind in enumerate(tqdm(indices_all)):\n",
    "        try:\n",
    "            _ = dataset.__getitem__(ind)\n",
    "        except RuntimeError:\n",
    "            exclude_inds.append(i)\n",
    "    \n",
    "    if len(exclude_inds)>0:\n",
    "        print('issue inds', exclude_inds, len(exclude_inds))\n",
    "        keep = np.setdiff1d(np.arange(indices_all.shape[0]), np.array(exclude_inds))\n",
    "        indices_all = indices_all[keep]\n",
    "        \n",
    "    np.save(dataroot + '/'+'2019_valid_indices.npy', indices_all)\n",
    "    \n",
    "    return indices_all\n",
    "    \n",
    "tapply = inaturalist_train()\n",
    "\n",
    "dataroot = '/lab/arios/data/inaturalist/'\n",
    "\n",
    "\n",
    "# indices_all = exclude_problem_indices_inaturalist_2019(dataroot, transform=tapply)\n",
    "# dset = datasets.INaturalist(dataroot, version='2019', target_type=['super'], \\\n",
    "#     transform=tapply, target_transform=None, download=False)\n",
    "\n",
    "\n",
    "dset= WrapNaturalist(dataroot, version='2021_train_mini', target_type=['phylum'], \\\n",
    "    transform=tapply, target_transform=None, download=False)\n",
    "\n",
    "\n",
    "# inds = np.load(dataroot+'/'+'2019_valid_indices.npy')\n",
    "\n",
    "\n",
    "print(dset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16506/758536857.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#     transform=tapply, target_transform=None, download=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ProjIntel/incDFM/notebooks/../src/inaturalist_dataset.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getindex__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/incDFM_proj/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attribute_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataroot = '/lab/arios/data/inaturalist/'\n",
    "class inaturalist_normalize():\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "\n",
    "\n",
    "class inaturalist_train():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.RandomHorizontalFlip(), TF.ToTensor(), \n",
    "                            #   inaturalist_normalize(),\n",
    "                              ])\n",
    "        # self.tf = TF.Compose([core50_normalize()])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "tapply = inaturalist_train()\n",
    "\n",
    "\n",
    "\n",
    "# dset = datasets.INaturalist(dataroot, version='2019', target_type=['super'], \\\n",
    "#     transform=tapply, target_transform=None, download=False)\n",
    "\n",
    "print(dset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Union, Tuple\n",
    "\n",
    "class WrapIndex(datasets.INaturalist):\n",
    "    \n",
    "    def __init__(self, root: str, version: str = \"2021_train\", target_type: Union[List[str], str] = \"full\", transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False) -> None:\n",
    "        super().__init__(root, version, target_type, transform, target_transform, download)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        cat_id, _ = self.index[index]\n",
    "\n",
    "        target = []\n",
    "        target_name = []\n",
    "        for t in self.target_type:\n",
    "            if t == \"full\":\n",
    "                target.append(cat_id)\n",
    "            else:\n",
    "                target.append(self.categories_map[cat_id][t])\n",
    "                if '21' in self.version:\n",
    "                    target_name.append(self.all_categories[cat_id].split('_')[2])\n",
    "        target = tuple(target) if len(target) > 1 else target[0]\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "\n",
    "        if '21' in self.version:\n",
    "            return target, target_name\n",
    "        \n",
    "        return target\n",
    "    \n",
    "    \n",
    "    def __getindices_2019__(self):\n",
    "        \n",
    "        self.labels = np.zeros((self.__len__(),)).astype(int)\n",
    "        for i in range(self.__len__()):\n",
    "            target = self.__getitem__(i)            \n",
    "            self.labels[i]=target\n",
    "            \n",
    "        return self.labels\n",
    "            \n",
    "            \n",
    "    def __getindices_2021__(self):\n",
    "        self.labels_map =[]\n",
    "        self.labels = np.zeros((self.__len__(),)).astype(int)\n",
    "        for i in range(self.__len__()):\n",
    "            target, name = self.__getitem__(i)            \n",
    "            self.labels[i]=target\n",
    "            \n",
    "            self.labels_map.append((target, name))\n",
    "            \n",
    "        self.labels_map = list(set(self.labels_map))\n",
    "        \n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    \n",
    "dset_index = WrapIndex(dataroot, version='2021_train_mini', target_type=['phylum'], \\\n",
    "    transform=tapply, target_transform=None, download=False)\n",
    "\n",
    "\n",
    "# dset_index = WrapIndex(dataroot, version='2019', target_type=['super'], \\\n",
    "#     transform=tapply, target_transform=None, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dset_index.__getindices_2019__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 2284, 3: 41204, 0: 3912, 5: 14513, 1: 47867, 4: 158463})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(dset_index, batch_size=100,shuffle=False, num_workers=4)\n",
    "y = np.zeros((dset_index.__len__(),)).astype(int)\n",
    "y_names = []\n",
    "y_match=[]\n",
    "for i, batch in enumerate(loader):\n",
    "    \n",
    "    # y_vals, y_n = batch\n",
    "    \n",
    "    y_vals= batch\n",
    "\n",
    "            \n",
    "    y[i*100:i*100+100]=y_vals.numpy()\n",
    "    \n",
    "    # y_names.extend(list(y_n[0]))\n",
    "    \n",
    "    \n",
    "    # y_match.extend(list(zip(y_vals.numpy().tolist(), list(y_n[0]))))\n",
    "    \n",
    "    \n",
    "    \n",
    "# list(set(y_match))\n",
    "    # break\n",
    "    \n",
    "# y_names\n",
    "\n",
    "np.unique(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cat_name,_ = dset.index[100]\n",
    "dset_index.categories_map[cat_name]['phylum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 'Ascomycota'),\n",
       " (12, 'Tracheophyta'),\n",
       " (9, 'Chlorophyta'),\n",
       " (3, 'Cnidaria'),\n",
       " (11, 'Rhodophyta'),\n",
       " (2, 'Chordata'),\n",
       " (7, 'Basidiomycota'),\n",
       " (4, 'Echinodermata'),\n",
       " (1, 'Arthropoda'),\n",
       " (0, 'Annelida'),\n",
       " (10, 'Marchantiophyta'),\n",
       " (8, 'Bryophyta'),\n",
       " (5, 'Mollusca')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(y_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 200,\n",
       "         1: 137600,\n",
       "         2: 120800,\n",
       "         3: 1300,\n",
       "         4: 1050,\n",
       "         5: 8450,\n",
       "         6: 4200,\n",
       "         7: 12850,\n",
       "         8: 1750,\n",
       "         9: 200,\n",
       "         10: 350,\n",
       "         11: 350,\n",
       "         12: 210900})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class inaturalist_normalize():\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "\n",
    "\n",
    "class inaturalist_train():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.RandomHorizontalFlip(), TF.ToTensor(), inaturalist_normalize()])\n",
    "        # self.tf = TF.Compose([core50_normalize()])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "    \n",
    "class inaturalist_test():\n",
    "    \"\"\"Pre-processing for inaturalist training.\n",
    "    \"\"\"\n",
    "    # TODO maybe the tranform topilimage is messing up the normalization!\n",
    "    def __init__(self):\n",
    "        self.tf = TF.Compose([TF.Resize(256), TF.CenterCrop(224), TF.ToTensor(), inaturalist_normalize()])\n",
    "        # self.tf = TF.Compose([core50_normalize()])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.tf(img)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataroot = '/lab/arios/data/inaturalist/'\n",
    "\n",
    "tapply = inaturalist_train()\n",
    "\n",
    "dset = datasets.INaturalist(dataroot, version= '2019', target_type= 'super', download=False, transform=tapply)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dset, batch_size=10, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 4, 4, 3, 4, 4, 4, 5, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cats = {0:0,1:0,2:0,3:0,4:0,5:0}\n",
    "\n",
    "for pair in dset.index:\n",
    "    cats[dset.categories_map[pair[0]]['super']]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1827.2, 456.8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2284*0.8, 2284*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19297/1822522658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268243"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c94a767549d1886d9c95c29bbf3f48fd10fe84ccdcfd75ddbc5ba7750ae70118"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('proj_intel': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

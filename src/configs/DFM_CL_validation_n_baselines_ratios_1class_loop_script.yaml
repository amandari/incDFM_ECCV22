#dataset + experiment if aLREADY PRE-SET 

# dset_name: cifar10
# num_tasks: 9
# experiment_filepath: ./Experiments_DFM_CL/cifar10/holdout_0.35_val_0.1/ # if already predetermined
# preload: False
# equalize_labels: False

# dset_name: cifar100
# num_tasks: 19
# experiment_filepath: ./Experiments_DFM_CL/cifar100/holdout_0.25_val_0.1/ # if already predetermined
# preload: False
# equalize_labels: False

# dset_name: svhn
# num_tasks: 9
# experiment_filepath: ./Experiments_DFM_CL/svhn/holdout_0.35_val_0.1/ # if already predetermined
# preload: False
# equalize_labels: False


dset_name: emnist
num_tasks: 25
experiment_filepath: ./Experiments_DFM_CL/emnist/holdout_0.25_val_0.1/ # if already predetermined
preload: False
equalize_labels: False


# dset_name: inaturalist
# num_tasks: 5
# experiment_filepath: ./Experiments_DFM_CL/inaturalist/holdout_0.2_val_0.1_equalize/ # if already predetermined
# preload: True
# equalize_labels: True


# experiment_name: holdout_0.35_val_0.1 ## if experiment_filepath does not exist, comment it out and generate with experiment_name. 

# gpu
device: 1
test_num: -1 # if debuging put integer > 0. else, -1 
percent_old_mix: 4 # Can be greater then 1.0 - percent which is comming from "old data" of all classes previously seen (will have to be limited by amount of old holdout/test old data available)
holdout_percent: 0.25 # (only when creating the experiment) if you are already passing a filepath, this parameter wont matter
train_clf: True #For odin you need to set to True. 

percentile_val_threshold: 85 # select 80th percentile as initial threshold. (95,85,75 - 65)


# -------- for incremental vs multitask------

# ------incremental
holdout: True # if False, use test set for representing "old" mixed samples 
max_holdout: 0.5 ## just means you cant holdout more then X percent by dataset
num_per_task: 1
num_classes_first: 2
shuffle_order: False

val_percent: 0.1 # if leaving part of train as validation set


#-------------------------------------------------------
# Algorithm for thresholding and selection

#-------Simple Thresholding
prediction_propagation: True # if using the predicted labels instead of Ground truth labels for fitting PCA continually
## only change the above two parameters
threshold_percentile: 50
max_iter_pseudolabel: 10
threshold_type: simple # simple or iter
alg_dfm: simple # either tug or simple
w_old_i: 0.5



# ---------incDFM
# prediction_propagation: True # if using the predicted labels instead of Ground truth labels for fitting PCA continually
# # only change the above two parameters
# threshold_percentile: 85
# max_iter_pseudolabel: 10
# threshold_type: iter # simple or iter
# alg_dfm: simple # either tug or simple
# w_old_i: 0.5


# -----------------------------------------------------------------------------------------
# Underlying Algorithm 

# ------- DFM 
# novelty_detector_name: dfm
# detector_params:
#   pca_level: 0.995
#   score_type: pca
#   n_components: None
#   n_percent_comp: 0.2
# experiment_name_plot: dfm


# ------ for ODIN
# novelty_detector_name: odin
# only_novelty: False
# detector_params:
#   similarity: cosine
#   noise_magnitude: 0.02
#   weight_decay: 0.0001
# experiment_name_plot: odin
# threshold_type: simple 

# ------ for Softmax
# novelty_detector_name: softmax
# only_novelty: False
# detector_params:
#   fake_param: 0
# experiment_name_plot: softmax
# threshold_type: simple 


# ------ MAHALANOBIS
novelty_detector_name: mahal
detector_params:
  num_components: 1600
  balance_classes: True # set false for cifar100, emnist with val 95 because not enough predicted new data
experiment_name_plot: mahal
threshold_type: simple 


# -------------------------------------- Not necessary to change -----------------------------------------

# ------------------------------------------
# ------------ if you are doing finetuning 
# ------------------------------------------
# If you want to train classifier:
coreset_size: 5000 # if using keep_all_data, put corset_size = 0. 


train_technique: 1
lr: 0.001 # ref is 0.001

max_batch_ratio: 0.85
type_batch_divide: proportional


fc_sizes: '4096' # layers of clf perceptron, if using a classifier, else just put an empty string ''
finetune_backbone: 'off' # freeze backbone (to finetune backbone put 'all')


# ----If keeping old samples in coreset (Memory)
keep_all_data: False
coreset_size_MB: -1 # -1 = Not defined
log_interval: 10


# ----for classifier or trainable backbone, if applicable
num_epochs: 20
batchsize: 100
batchsize_test: 100
cut_epoch_short: False
weight_old: 0.5

#train_technique - scheduling mode 1 - ADAM + ReduceLROnPlateau
schedule_patience_perepoch: 0.25
schedule_decay: 0.5

#train_technique - scheduling mode 1 - SGD + StepLR
gamma_step_lr: 0.1
step_size_epoch_lr: 7 # in number of epochs 

# ---------------------
# ------------ Others
# ---------------------
# ----specifically for cifar100
num_tasks_cifar: 20 #fine = max is 100. 
type_l_cifar: super # if using superclasses put super. 

# ----others
num_threads: 6
num_workers: 4



#----------------------------------Do not touch-------------------------------------------------

## remove these params after
compute_score_name: Score_Simple
w_score: 0.5
th_percentile_confidence: -1
metric_confidence: variance
metric_confidence_direction_novel: max



# dont change these next 3 for anything other then core50
scenario: nc
scenario_classif: class
exp_type: class



# ---- paths + output
data_dir: ../../data
dir_results: ./Results_DFM_CL
# experiment_dir: ../experiments
experiment_dir: ./Experiments_DFM_CL



# ----- feature extraction
net_type: resnet50_contrastive 
dfm_layers_input: base.8 # consider making it a list type if concatenation is involved 
clf_layers_input: base.8
dfm_layers_factors: -1
use_image_as_input: False


# ---------------------------------------------------------------------

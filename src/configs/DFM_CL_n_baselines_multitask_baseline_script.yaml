#dataset + experiment if aLREADY PRE-SET 
# dset_name: emnist
# num_tasks: 1
# num_per_task: 26
# experiment_name: multitask
# num_classes_first: 26
# preload: False
# equalize_labels: False

# dset_name: cifar10
# num_tasks: 1
# num_per_task: 10
# experiment_name: multitask
# num_classes_first: 10
# preload: False
# equalize_labels: False

# dset_name: cifar100
# num_tasks: 1
# num_per_task: 20
# experiment_name: multitask
# num_classes_first: 20
# preload: False
# equalize_labels: False

dset_name: svhn
num_tasks: 1
num_per_task: 10
experiment_name: multitask
num_classes_first: 10
preload: False
equalize_labels: False



# gpu
device: 0
# ---- experiment specs (need to comment out experiment_filepath)
prediction_propagation: False # if using the predicted labels instead of Ground truth labels for fitting PCA continually
percentile_val_threshold: 95 # select 80th percentile as initial threshold. 
threshold_percentile: 50
val_percent: 0.1 # if leaving part of train as validation set
holdout_percent: 0.2 # (only when creating the experiment) if you are already passing a filepath, this parameter wont matter
max_iter_pseudolabel: 10
threshold_type: simple # simple or iter
alg_dfm: simple # either tug or simple
w_old_i: 0.5






compute_score_name: Score_Simple
w_score: 0.5
th_percentile_confidence: -1
metric_confidence: variance
metric_confidence_direction_novel: max
max_threshold_total: 50





holdout: True # if False, use test set for representing "old" mixed samples 
percent_old_mix: 1.0 # Can be greater then 1.0 - percent which is comming from "old data" of all classes previously seen (will have to be limited by amount of old holdout/test old data available)
max_holdout: 0.5 ## just means you cant holdout more then X percent by dataset
shuffle_order: False



# dont change these next 3 for anything other then core50
scenario: nc
scenario_classif: class
exp_type: class



# ---- paths + output
data_dir: ../../data
dir_results: ./Results_DFM_CL
# experiment_dir: ../experiments
experiment_dir: ./Experiments_DFM_CL
test_num: -1 # if debuging put integer > 0. else, -1 



# ----- feature extraction
# net_type: wide_resnet50_2_contrastive
# dfm_layers_input: base.8.2
# clf_layers_input: base.8.2

net_type: resnet50_contrastive
dfm_layers_input: base.8 # consider making it a list type if concatenation is involved 
clf_layers_input: base.8
dfm_layers_factors: -1
use_image_as_input: False



# ------- DFM 
novelty_detector_name: dfm
detector_params:
  pca_level: 0.995
  score_type: pca
  n_components: None
  n_percent_comp: 0.2
experiment_name_plot: dfm


# ------ for ODIN
# novelty_detector_name: odin
# only_novelty: False
# detector_params:
#   similarity: cosine
#   noise_magnitude: 0.02
#   weight_decay: 0.0001
# experiment_name_plot: odin
# threshold_type: simple 


# ------ MAHALANOBIS
# novelty_detector_name: mahal
# detector_params:
#   num_components: 1600
#   balance_classes: True
# experiment_name_plot: mahal
# threshold_type: simple 



# ------------------------------------------
# ------------ if you are doing finetuning 
# ------------------------------------------
train_clf: True

train_technique: 1
lr: 0.001 # ref is 0.001


fc_sizes: '4096' # layers of clf perceptron, if using a classifier, else just put an empty string ''
finetune_backbone: 'off' # freeze backbone, off


# ----If keeping old samples in coreset (Memory)
keep_all_data: False
coreset_size: 5000 # if using keep_all_data, put corset_size = 0. 
coreset_size_MB: -1 # -1 = Not defined
log_interval: 10


# ----for classifier or trainable backbone, if applicable
num_epochs: 20
batchsize: 50
batchsize_test: 100
cut_epoch_short: False
weight_old: 0.5

#train_technique - scheduling mode 1 - ADAM + ReduceLROnPlateau
schedule_patience_perepoch: 0.25
schedule_decay: 0.5

#train_technique - scheduling mode 1 - SGD + StepLR
gamma_step_lr: 0.1
step_size_epoch_lr: 7 # in number of epochs 

# ---------------------
# ------------ Others
# ---------------------
# ----specifically for cifar100
num_tasks_cifar: 20 #fine = max is 100. 
type_l_cifar: super # if using superclasses put super. 


# ----others
num_threads: 6
num_workers: 4
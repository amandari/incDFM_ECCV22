# dataset + experiment if aLREADY PRE-SET 
dset_name: cifar10
# experiment_filepath: ./experiments/svhn/inc_1class # if already predetermined


# gpu
device: 6

# ---- experiment specs (need to comment out experiment_filepath)
scenario: nc
scenario_classif: class
exp_type: class
# dont change the above 3 for svhn, cifar. 
num_per_task: 1
num_classes_first: 2
num_tasks: 9
experiment_name: inc_1class
shuffle_order: False



# ---- paths + output
data_dir: ../data
dir_results: ../src/novelty_dfm_CL/Results_DFM_CL
experiment_dir: ../experiments
test_num: -1 # if debuging put integer > 0. else, -1 



# ----- feature extraction
net_type: resnet50_contrastive
dfm_layers_input: base.8 # consider making it a list type if concatenation is involved 
clf_layers_input: base.8
dfm_layers_factors: -1
fc_sizes: '' # layers of clf perceptron, if using a classifier, else just put an empty string ''
finetune_backbone: 'off'
train_clf: False
use_image_as_input: False



# ------- DFM 
novelty_detector_name: dfm
detector_params:
  pca_level: 0.995
  score_type: pca
  n_components: None
  n_percent_comp: 0.2
experiment_name_plot: dfm






# ------------ if you are doing finetuning 


# ----If keeping old samples in coreset (Memory)
keep_all_data: False
coreset_size: 1000 # if using keep_all_data, put corset_size = 0. 
coreset_size_MB: -1 # -1 = Not defined
log_interval: 10


# ----for classifier or trainable backbone, if applicable
lr: 0.001
schedule_patience_perepoch: 0.25
schedule_decay: 0.5
batchsize: 50
batchsize_test: 100
num_epochs: 10
cut_epoch_short: False




# ----specifically for cifar100
num_tasks_cifar: 20 #fine = max is 100. 
type_l_cifar: fine # if using superclasses put super. 



# ----others
num_threads: 6
num_workers: 4
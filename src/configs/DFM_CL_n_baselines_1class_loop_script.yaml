#dataset + experiment if aLREADY PRE-SET 


dset_name: cifar10
num_tasks: 9
experiment_filepath: ./Experiments_DFM_CL/cifar10/inc_1class_holdout_hold_0.2_mix_1.0_correct/ # if already predetermined

# dset_name: cifar100
# num_tasks: 19
# experiment_filepath: ./Experiments_DFM_CL/cifar100/holdout_0.2_1.0/ # if already predetermined
# preload: False


# dset_name: svhn
# num_tasks: 9
# experiment_filepath: ./Experiments_DFM_CL/svhn/holdout_0.2_1.0/ # if already predetermined
# preload: False

# dset_name: emnist
# num_tasks: 25
# experiment_filepath: ./Experiments_DFM_CL/emnist/holdout_0.2_1.0/ # if already predetermined
# preload: False

preload: False
equalize_labels: False

# dset_name: inaturalist
# num_tasks: 5
# equalize_labels: True
# experiment_filepath: ./Experiments_DFM_CL/inaturalist/holdout_0.2_1.0_equalize/ # if already predetermined
# preload: True

# experiment_name: holdout_0.2_1.0_equalize



# gpu
device: 0
# ---- experiment specs (need to comment out experiment_filepath)
prediction_propagation: True # if using the predicted labels instead of Ground truth labels for fitting PCA continually
threshold_type: iter # simple or iter
alg_dfm: simple # either tug or simple
w_old_i: 0.5
threshold_percentile: 85
max_threshold_total: 50
compute_score_name: Score_Simple
max_iter_pseudolabel: 8
w_score: 0.5
th_percentile_confidence: -1
metric_confidence: variance
metric_confidence_direction_novel: max


holdout: True # if False, use test set for representing "old" mixed samples 
holdout_percent: 0.2 # (only when creating the experiment) if you are already passing a filepath, this parameter wont matter
percent_old_mix: 1.0 # Can be greater then 1.0 - percent which is comming from "old data" of all classes previously seen (will have to be limited by amount of old holdout/test old data available)
max_holdout: 0.5 ## just means you cant holdout more then X percent by dataset
num_per_task: 1
num_classes_first: 2
shuffle_order: False



# dont change these next 3 for anything other then core50
scenario: nc
scenario_classif: class
exp_type: class



# ---- paths + output
data_dir: ../../data
dir_results: ./Results_DFM_CL
# experiment_dir: ../experiments
experiment_dir: ./Experiments_DFM_CL
test_num: -1 # if debuging put integer > 0. else, -1 



# ----- feature extraction
net_type: resnet50_contrastive 
dfm_layers_input: base.8 # consider making it a list type if concatenation is involved 
clf_layers_input: base.8
dfm_layers_factors: -1
use_image_as_input: False



# ------- DFM 
novelty_detector_name: dfm
detector_params:
  pca_level: 0.995
  score_type: pca
  n_components: None
  n_percent_comp: 0.2
experiment_name_plot: dfm


# ------ for ODIN
# novelty_detector_name: odin
# only_novelty: False
# detector_params:
#   similarity: cosine
#   noise_magnitude: 0.02
#   weight_decay: 0.0001
# experiment_name_plot: odin
# threshold_type: simple 


# ------ MAHALANOBIS
# novelty_detector_name: mahal
# detector_params:
#   num_components: 1600
#   balance_classes: True
# experiment_name_plot: mahal
# threshold_type: simple 



# ------------------------------------------
# ------------ if you are doing finetuning 
# ------------------------------------------
train_clf: False

train_technique: 1
lr: 0.001 # ref is 0.001


fc_sizes: '4096' # layers of clf perceptron, if using a classifier, else just put an empty string ''
finetune_backbone: 'off' # freeze backbone


# ----If keeping old samples in coreset (Memory)
keep_all_data: False
coreset_size: 5000 # if using keep_all_data, put corset_size = 0. 
coreset_size_MB: -1 # -1 = Not defined
log_interval: 10


# ----for classifier or trainable backbone, if applicable
num_epochs: 20
batchsize: 50
batchsize_test: 100
cut_epoch_short: False
weight_old: 0.5

#train_technique - scheduling mode 1 - ADAM + ReduceLROnPlateau
schedule_patience_perepoch: 0.25
schedule_decay: 0.5

#train_technique - scheduling mode 1 - SGD + StepLR
gamma_step_lr: 0.1
step_size_epoch_lr: 7 # in number of epochs 

# ---------------------
# ------------ Others
# ---------------------
# ----specifically for cifar100
num_tasks_cifar: 20 #fine = max is 100. 
type_l_cifar: super # if using superclasses put super. 

# ----others
num_threads: 6
num_workers: 4
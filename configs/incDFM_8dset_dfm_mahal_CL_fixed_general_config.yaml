# Mostly standard configuration - Usually do not have to alter
incdfm_config: ../configs_new/ood_method_for_8dset/incdfm_ood_config.yaml
dfm_config: ../configs_new/ood_method_for_8dset/dfm_ood_config.yaml
mahal_config: ../configs_new/ood_method_for_8dset/mahal_ood_config.yaml
softmax_config: ../configs_new/ood_method_for_8dset/softmax_ood_config.yaml
odin_config: ../configs_new/ood_method_for_8dset/odin_ood_config.yaml


#-------------------------------------------------------
# Experiment setup 
seed: 0
holdout: True # if False, use test set for representing "old" mixed samples 


# ----------------------------
# if you are doing finetuning 
train_technique: 1
lr: 0.001 # ref is 0.001
fc_sizes: '4096' # layers of clf perceptron, if using a classifier, else just put an empty string ''

batchsize_test: 100
max_batch_ratio: 0.85
type_batch_divide: proportional

finetune_backbone: 'off' # freeze backbone (to finetune backbone put 'all')

# ----If keeping old samples in coreset (Memory)
log_interval: 10

# ----for classifier or trainable backbone, if applicable
batchsize: 100
cut_epoch_short: False
weight_old: 0.5

#train_technique - scheduling mode 1 - ADAM + ReduceLROnPlateau
schedule_patience_perepoch: 0.25
schedule_decay: 0.5

#train_technique - scheduling mode 1 - SGD + StepLR
gamma_step_lr: 0.1
step_size_epoch_lr: 7 # in number of epochs 


# ----others
num_threads: 6
num_workers: 4

#----------------------------------Do not touch-------------------------------------------------

## remove these params after
compute_score_name: Score_Simple
w_score: 0.5
th_percentile_confidence: -1
metric_confidence: variance
metric_confidence_direction_novel: max


experiment_name_plot: OOD

# ----- feature extraction
net_type: resnet50_contrastive 
dfm_layers_input: base.8 # consider making it a list type if concatenation is involved 
clf_layers_input: base.8
dfm_layers_factors: -1
use_image_as_input: False


# ---------------------------------------------------------------------
